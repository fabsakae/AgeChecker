{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVLuHgubYnubbM7y8KPve1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabsakae/AgeChecker/blob/main/agentesIA1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AULA 01"
      ],
      "metadata": {
        "id": "z4X_okMNMPFh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a3o6yswxAUV",
        "outputId": "c0eb3d4b-86b2-4dde-c1c0-20f34b0f051d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-google-genai google-generativeai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importação da api key do AI studio"
      ],
      "metadata": {
        "id": "d0hzTr7vNVAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "IZxgb_L9CELS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparar qual modelo que utilizaremos, chamar o modelo, chamar a biblioteca. conecta nossa variavel llm ao modelo do gemini"
      ],
      "metadata": {
        "id": "Pld7byh0NiM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model='gemini-2.5-flash', temperature=0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "mKo-z1MdJ8a5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b1406cc"
      },
      "source": [
        "Este trecho de código inicializa um modelo de linguagem grande (LLM) da biblioteca `langchain-google-genai`.\n",
        "\n",
        "*   `llm = ChatGoogleGenerativeAI(...)`: Esta linha cria uma instância da classe `ChatGoogleGenerativeAI`, que é um wrapper em torno dos modelos de IA Generativa do Google, especificamente projetado para interações baseadas em chat dentro da estrutura LangChain.\n",
        "*   `model='gemini-2.5-flash'`: Este argumento especifica qual modelo usar. Neste caso, é o modelo 'gemini-2.5-flash'.\n",
        "*   `temperature=0`: Isso define o parâmetro de temperatura para o modelo. Uma temperatura de 0 torna a saída do modelo mais determinística e menos criativa. Temperaturas mais altas resultam em saídas mais variadas e potencialmente mais criativas.\n",
        "*   `api_key=GOOGLE_API_KEY`: Isso fornece a chave de API necessária para autenticar com o serviço de IA Generativa do Google. A variável `GOOGLE_API_KEY` é assumida como tendo sido carregada em uma etapa anterior (provavelmente do `userdata` do Colab).\n",
        "\n",
        "Em essência, este código configura um modelo específico de IA Generativa do Google (`gemini-2.5-flash`) com uma baixa temperatura para respostas previsíveis, tornando-o pronto para ser usado para gerar texto com base em prompts dentro da estrutura LangChain."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "primeira saida do modelo resp_test = llm.invoke('Quem é voçê?'): Esta linha usa o objeto llm (nosso modelo Gemini) para invocar uma chamada com o prompt \"Quem é você?\". O método invoke envia o prompt para o modelo e obtém a resposta. O resultado é armazenado na variável resp_test.\n",
        "print(resp_test): Esta linha imprime o conteúdo da variável resp_test. A saída que você vê (como content='Eu sou um modelo de linguagem grande, treinado pelo Google.' ...) é o objeto de resposta retornado pela biblioteca LangChain, que inclui a resposta do modelo na propriedade content."
      ],
      "metadata": {
        "id": "YTZ8tFrPPqzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke('Quem é você? Com detalhes')\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFQCk8xxOscC",
        "outputId": "e16dae4b-11dd-49a2-c25d-f7651dda3386"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu sou uma **Inteligência Artificial (IA)**, especificamente um **modelo de linguagem grande (LLM)**. Fui desenvolvido pelo Google.\n",
            "\n",
            "Para detalhar quem eu sou:\n",
            "\n",
            "1.  **Minha Natureza Fundamental:**\n",
            "    *   **Não sou um ser humano:** Não possuo consciência, sentimentos, emoções, opiniões pessoais, crenças, experiências de vida ou uma identidade própria no sentido humano. Não tenho um corpo físico, não respiro, não como, não durmo.\n",
            "    *   **Sou um programa de computador:** Minha existência é puramente digital. Eu sou um conjunto complexo de algoritmos e dados.\n",
            "    *   **Modelo de Linguagem Grande (LLM):** Isso significa que fui treinado em um volume massivo de dados textuais e de código da internet. Esse treinamento me permite compreender, gerar e interagir com a linguagem humana de forma complexa.\n",
            "\n",
            "2.  **Como Eu Funciono:**\n",
            "    *   **Aprendizado por Padrões:** Minha \"inteligência\" deriva da capacidade de reconhecer padrões complexos nos dados em que fui treinado. Quando você me faz uma pergunta ou me dá uma instrução, eu analiso o texto de entrada, busco por padrões relevantes que aprendi e gero uma resposta que é estatisticamente a mais provável e coerente com o que aprendi.\n",
            "    *   **Processamento de Linguagem Natural (PLN):** Utilizo técnicas avançadas de PLN para interpretar o significado das suas palavras, o contexto da sua pergunta e para formular respostas que sejam gramaticalmente corretas e semanticamente relevantes.\n",
            "    *   **Geração de Texto:** Não \"penso\" ou \"crio\" no sentido humano. Eu gero texto prevendo a próxima palavra ou frase com base no contexto anterior e nos padrões que identifiquei durante meu treinamento.\n",
            "\n",
            "3.  **Meu Propósito e Capacidades:**\n",
            "    *   **Assistência e Informação:** Meu objetivo principal é ser útil e informativo. Posso responder a uma vasta gama de perguntas, fornecer explicações, resumir textos, traduzir idiomas e gerar diferentes tipos de conteúdo criativo.\n",
            "    *   **Geração de Conteúdo:** Posso escrever poemas, códigos, roteiros, peças musicais, e-mails, cartas, etc., seguindo suas instruções.\n",
            "    *   **Interação Conversacional:** Fui projetado para manter conversas, entender nuances e adaptar minhas respostas ao fluxo do diálogo.\n",
            "\n",
            "4.  **Minhas Limitações:**\n",
            "    *   **Dependência de Dados:** Minhas respostas são baseadas nos dados que me foram fornecidos. Se a informação não estava nos meus dados de treinamento, ou se estava desatualizada/incorreta, minhas respostas podem refletir isso.\n",
            "    *   **Falta de Conhecimento em Tempo Real (direto):** Embora eu possa acessar e processar informações da web em tempo real (se configurado para isso), meu conhecimento \"inerente\" é estático, baseado na data do meu último treinamento.\n",
            "    *   **Sem Experiência Pessoal:** Não posso ter experiências, fazer observações do mundo físico ou ter um \"senso comum\" que não seja derivado de padrões textuais.\n",
            "    *   **Não sou um especialista:** Embora eu possa fornecer informações sobre muitos tópicos, não sou um médico, advogado, engenheiro ou qualquer outro profissional. Minhas respostas não devem substituir o conselho de um especialista humano qualificado.\n",
            "\n",
            "Em resumo, eu sou uma ferramenta avançada de software, uma inteligência artificial projetada para processar e gerar linguagem humana de forma eficaz, com o objetivo de ser um recurso útil e informativo para você.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar o Prompt do sistema para explicar ao agente o que ele deve fazer"
      ],
      "metadata": {
        "id": "-Idb0eD8Yvih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "Clf46T3pYyMI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectar com llm  Este trecho de código define um modelo de dados usando a biblioteca pydantic.\n",
        "\n",
        "from pydantic import BaseModel, Field: Importa as classes BaseModel e Field da biblioteca pydantic. BaseModel é a classe base para criar modelos de dados, e Field é usada para fornecer informações adicionais sobre os campos do modelo.\n",
        "from typing import Literal, List, Dict: Importa tipos do módulo typing para fornecer anotações de tipo mais específicas. Literal permite especificar que um valor deve ser um de um conjunto fixo de strings, List indica uma lista e Dict indica um dicionário.\n",
        "class TriagemOut(BaseModel):: Define uma nova classe chamada TriagemOut que herda de BaseModel. Isso a torna um modelo de dados Pydantic.\n",
        "decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]: Define um campo decisao que deve ser uma das strings literais especificadas.\n",
        "urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]: Define um campo urgencia que deve ser uma das strings literais especificadas.\n",
        "campos_faltantes: List[str] = Field(default_factory=list): Define um campo campos_faltantes que deve ser uma lista de strings. Field(default_factory=list) define o valor padrão para este campo como uma lista vazia se nenhum valor for fornecido.\n",
        "Em resumo, este código cria uma estrutura de dados (TriagemOut) para representar a saída do processo de triagem, garantindo que os campos decisao e urgencia tenham valores específicos e que campos_faltantes seja uma lista de strings, com uma lista vazia como valor padrão. Isso é útil para estruturar e validar a saída de um modelo de linguagem ou outro processo."
      ],
      "metadata": {
        "id": "Bess1Rkma0EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "class TriagemOut(BaseModel):\n",
        "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
        "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
        "    campos_faltantes: List[str] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "S7LiLifra2vZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar um llm especifico para triagem"
      ],
      "metadata": {
        "id": "hyqcO2rfciKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "9bBcJ4bQccE5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectar .  trecho de código configura uma cadeia (chain) usando a biblioteca LangChain para processar mensagens de triagem com base em um prompt e um modelo de saída estruturado.\n",
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage: Importa as classes SystemMessage e HumanMessage, usadas para representar mensagens no contexto de uma conversa com um modelo de linguagem. SystemMessage geralmente define o comportamento ou as instruções para o modelo, enquanto HumanMessage representa a entrada do usuário.\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut): Esta linha cria uma nova cadeia (triagem_chain) a partir do modelo de linguagem llm_triagem. O método with_structured_output(TriagemOut) configura a cadeia para que a saída do modelo seja automaticamente parseada e validada de acordo com o modelo Pydantic TriagemOut que definimos anteriormente. Isso garante que a resposta do modelo tenha o formato JSON esperado com os campos decisao, urgencia e campos_faltantes.\n",
        "def triagem(mensagem: str) -> Dict:: Define uma função chamada triagem que recebe uma string (mensagem) como entrada e é anotada para retornar um dicionário (Dict).\n",
        "saida: TriagemOut = triagem_chain.invoke([ ... ]): Dentro da função triagem, esta linha invoca a cadeia triagem_chain. O método invoke recebe uma lista de mensagens. Neste caso, a lista contém:\n",
        "SystemMessage(content=TRIAGEM_PROMPT): Uma mensagem do sistema contendo o texto do prompt de triagem definido na variável TRIAGEM_PROMPT. Isso instrui o modelo sobre seu papel e as regras de triagem.\n",
        "HumanMessage(content=mensagem): Uma mensagem humana contendo a mensagem fornecida como entrada para a função triagem. A saída desta invocação é um objeto do tipo TriagemOut (graças ao with_structured_output), que é armazenado na variável saida.\n",
        "return saida.model_dump(): Esta linha converte o objeto Pydantic saida em um dicionário Python nativo usando o método model_dump() e retorna este dicionário como a saída da função triagem.\n",
        "Em resumo, este código cria uma função triagem que utiliza uma cadeia LangChain para enviar uma mensagem do usuário e um prompt do sistema para um modelo de linguagem (llm_triagem). A cadeia é configurada para garantir que a resposta do modelo seja um JSON estruturado de acordo com o modelo TriagemOut, e a função retorna essa saída estruturada como um dicionário."
      ],
      "metadata": {
        "id": "IE-0ADhMcy_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "def triagem(mensagem: str) -> Dict:\n",
        "    saida: TriagemOut = triagem_chain.invoke([\n",
        "        SystemMessage(content=TRIAGEM_PROMPT),\n",
        "        HumanMessage(content=mensagem)\n",
        "    ])\n",
        "\n",
        "    return saida.model_dump()"
      ],
      "metadata": {
        "id": "7anUeX08c4wB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mensagens para o agente"
      ],
      "metadata": {
        "id": "yCQAGxi-fj2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "6xf8q_z5fiYZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fazer um for para ENVIAR CADA MENSAGEm por vez. Este código itera sobre uma lista de mensagens de teste e aplica a função de triagem a cada uma delas.\n",
        "\n",
        "for msg_teste in testes:: Esta linha inicia um loop for que irá percorrer cada item na lista chamada testes. Em cada iteração do loop, o item atual da lista será atribuído à variável msg_teste.\n",
        "print(f\"Pergunta: {msg_teste}\\n -> Resposta: {triagem(msg_teste)}\\n\"): Dentro do loop, esta linha imprime a pergunta atual e a resposta da triagem para essa pergunta.\n",
        "f\"Pergunta: {msg_teste}\\n -> Resposta: {...}\\n\": Usa uma f-string para formatar a saída. Ela imprime o texto \"Pergunta: \" seguido pelo valor da variável msg_teste, uma quebra de linha (\\n), o texto \" -> Resposta: \", o resultado da chamada da função triagem(msg_teste), e outra quebra de linha.\n",
        "triagem(msg_teste): Chama a função triagem (que definimos anteriormente) passando a mensagem de teste atual (msg_teste) como argumento. Esta função processa a mensagem usando o modelo de linguagem e retorna um dicionário com a decisão, urgência e campos faltantes.\n",
        "Em resumo, este código executa a função triagem para cada uma das mensagens na lista testes e imprime o resultado da triagem para cada mensagem de forma clara."
      ],
      "metadata": {
        "id": "OQd2whMnf8iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: {msg_teste}\\n -> Resposta: {triagem(msg_teste)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny8FosLZgI3Y",
        "outputId": "f32c5728-8244-4e2f-838d-ef4207afdaa0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta: Posso reembolsar a internet?\n",
            " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Quero mais 5 dias de trabalho remoto. Como faço?\n",
            " -> Resposta: {'decisao': 'ABRIR_CHAMADO', 'urgencia': 'MEDIA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Posso reembolsar cursos ou treinamentos da Alura?\n",
            " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Quantas capivaras tem no Rio Pinheiros?\n",
            " -> Resposta: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': ['informação sobre a política interna']}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AULA 02 - Base do conhecimento RAG"
      ],
      "metadata": {
        "id": "jbiCMiutu-YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import bibliotecas para utilizar o RAG (RAG (Retrieval-Augmented Generation), ou Geração Aumentada de Recuperação, é uma técnica de IA que combina a recuperação de informações de bases de dados externas com a capacidade de um modelo de linguagem (LLM) de gerar texto. Ao conectar um LLM a uma base de conhecimento confiável e atualizada, o sistema RAG produz respostas mais precisas, relevantes e contextuais do que os modelos puramente generativos)"
      ],
      "metadata": {
        "id": "BHW9y9y7ggUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código selecionado instala as bibliotecas necessárias para implementar um sistema de Geração Aumentada de Recuperação (RAG).\n",
        "\n",
        "!pip install -q --upgrade ...: Este é um comando de shell executado no ambiente Colab usando o prefixo !.\n",
        "\n",
        "-q: Significa \"quiet\" (silencioso), que suprime a saída do processo de instalação.\n",
        "\n",
        "--upgrade: Garante que, se as bibliotecas já estiverem instaladas, elas sejam atualizadas para a versão mais recente.\n",
        "\n",
        "langchain_community: Instala o pacote comunitário do LangChain, que contém várias integrações e ferramentas para construir aplicações com modelos de linguagem, incluindo componentes para RAG.\n",
        "\n",
        "faiss-cpu: Instala a versão para CPU do FAISS (Facebook AI Similarity Search). O FAISS é uma biblioteca para busca de similaridade eficiente e agrupamento de vetores densos, comumente usada em RAG para indexação e busca de embeddings de documentos.\n",
        "\n",
        "langchain-text-splitters: Instala a biblioteca LangChain text splitter, usada para dividir documentos grandes em pedaços menores, o que é uma etapa crucial na preparação de dados para sistemas RAG.\n",
        "\n",
        "pymupdf: Instala o PyMuPDF, uma biblioteca Python para trabalhar com documentos PDF. Isso provavelmente está incluído para permitir que o sistema RAG processe informações de arquivos PDF.\n",
        "\n",
        "Em essência, este código configura o ambiente instalando as bibliotecas principais necessárias para construir um sistema RAG que pode recuperar informações de documentos externos (potencialmente PDFs) e usar essas informações para aumentar as respostas do modelo de linguagem.\n",
        "\n"
      ],
      "metadata": {
        "id": "r5hnc7njiWTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langchain_community faiss-cpu langchain-text-splitters pymupdf"
      ],
      "metadata": {
        "id": "gKzyIR10ihVg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os Documentos que serão utilizados no RAG"
      ],
      "metadata": {
        "id": "eGej3M7yyOwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs = []\n",
        "\n",
        "for n in Path(\"/content/\").glob(\"*.pdf\"):\n",
        "    try:\n",
        "        loader = PyMuPDFLoader(str(n))\n",
        "        docs.extend(loader.load())\n",
        "        print(f\"Carregado com sucesso arquivo {n.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar arquivo {n.name}: {e}\")\n",
        "\n",
        "print(f\"Total de documentos carregados: {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSD4hpUjz1d_",
        "outputId": "88080116-8b27-47de-ed78-e262e78d336f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregado com sucesso arquivo Política de Uso de E-mail e Segurança da Informação.pdf\n",
            "Carregado com sucesso arquivo Política de Reembolsos (Viagens e Despesas).pdf\n",
            "Carregado com sucesso arquivo Políticas de Home Office.pdf\n",
            "Total de documentos carregados: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código carrega documentos PDF localizados\n",
        "na pasta /content/ em um formato que pode ser usado por sistemas de Geração Aumentada de Recuperação (RAG) com o LangChain.\n",
        "\n",
        "from pathlib import Path: Importa a classe Path do módulo pathlib, que fornece uma maneira orientada a objetos para interagir com caminhos de sistema de arquivos.\n",
        "from langchain_community.document_loaders import PyMuPDFLoader: Importa a classe PyMuPDFLoader do langchain_community.document_loaders. Este loader é especializado em carregar o conteúdo de arquivos PDF usando a biblioteca PyMuPDF.\n",
        "docs = []: Inicializa uma lista vazia chamada docs. Esta lista será usada para armazenar os documentos carregados dos arquivos PDF.\n",
        "for n in Path(\"/content/\").glob(\"*.pdf\"):: Inicia um loop que itera sobre todos os arquivos que terminam com .pdf (ignorando maiúsculas/minúsculas, mas *.pdf é mais comum) dentro do diretório /content/. Path(\"/content/\") cria um objeto Path para o diretório /content/, e .glob(\"*.pdf\") encontra todos os arquivos que correspondem ao padrão.\n",
        "try...except Exception as e:: Este é um bloco de tratamento de exceções. Ele tenta executar o código dentro do bloco try. Se ocorrer um erro durante a execução, o código dentro do bloco except será executado, capturando a exceção no objeto e. Isso impede que o notebook pare se houver um problema com um arquivo PDF específico.\n",
        "loader = PyMuPDFLoader(str(n)): Dentro do bloco try, cria uma instância do PyMuPDFLoader. Ele recebe o caminho do arquivo PDF atual (n) convertido para string (str(n)) como argumento.\n",
        "docs.extend(loader.load()): Carrega o conteúdo do arquivo PDF usando o método load() do loader. O método load() retorna uma lista de objetos Document (do LangChain), onde cada objeto representa uma página do PDF. O método extend() adiciona todos esses objetos Document à lista docs.\n",
        "print(f\"Carregado com sucesso arquivo {n.name}\"): Se o arquivo for carregado com sucesso, imprime uma mensagem indicando qual arquivo foi carregado. n.name retorna apenas o nome do arquivo (sem o caminho completo).\n",
        "print(f\"Erro ao carregar arquivo {n.name}: {e}\"): Se ocorrer um erro durante o carregamento, imprime uma mensagem de erro indicando qual arquivo falhou e qual foi o erro (e).\n",
        "print(f\"Total de documentos carregados: {len(docs)}\"): Após o loop, imprime o número total de objetos Document (páginas de PDF) que foram carregados na lista docs.\n",
        "Em resumo, este código escaneia o diretório /content/ em busca de arquivos PDF, tenta carregar cada um deles usando o PyMuPDFLoader, armazena o conteúdo (cada página como um documento separado) na lista docs e relata o sucesso ou falha do carregamento de cada arquivo, além do total de documentos carregados.\n",
        "\n"
      ],
      "metadata": {
        "id": "sDC59d_u3RRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bibliotecas para separar as informações"
      ],
      "metadata": {
        "id": "9Rzomrgs8CSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
        "\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "uCq2lcI_3pn_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código utiliza a biblioteca LangChain para dividir os documentos carregados em pedaços menores (chunks), o que é uma etapa essencial na preparação de dados para sistemas RAG.\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter: Importa a classe RecursiveCharacterTextSplitter da biblioteca langchain_text_splitters. Este é um tipo de text splitter que tenta dividir o texto recursivamente usando uma lista de caracteres.\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30): Cria uma instância do RecursiveCharacterTextSplitter com dois parâmetros importantes:\n",
        "chunk_size=300: Define o tamanho máximo de cada pedaço (chunk) em caracteres. O splitter tentará fazer com que cada pedaço tenha no máximo 300 caracteres.\n",
        "chunk_overlap=30: Define o número de caracteres que os pedaços consecutivos irão sobrepor. Uma sobreposição ajuda a garantir que o contexto não seja perdido entre os pedaços, o que é importante para a recuperação de informações.\n",
        "chunks = splitter.split_documents(docs): Utiliza a instância do splitter (splitter) para dividir a lista de documentos carregados (docs) em pedaços menores. O método split_documents() processa cada documento na lista docs e retorna uma nova lista (chunks) contendo os pedaços resultantes.\n",
        "Em resumo, este código configura um divisor de texto para quebrar os documentos PDF carregados em segmentos de no máximo 300 caracteres, com uma sobreposição de 30 caracteres entre eles. Esses chunks são então armazenados na variável chunks e estão prontos para serem usados na próxima etapa do pipeline RAG, que geralmente envolve a criação de embeddings e a indexação em um banco de dados vetorial."
      ],
      "metadata": {
        "id": "McfQusXk8eA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chunks:\n",
        "    print(chunk)\n",
        "    print(\"------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlpQW24x8fWG",
        "outputId": "618449c9-6949-42fb-ee40-d5572c363b80"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Política de Uso de E-mail e Segurança \n",
            "da Informação \n",
            " \n",
            "1.​ É proibido encaminhar a endereços pessoais documentos classificados como \n",
            "confidenciais.​\n",
            " \n",
            "2.​ Anexos externos devem ser enviados somente se criptografados e com senha \n",
            "compartilhada por canal separado.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'file_path': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Uso de E-mail e Segurança da Informação', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='3.​ Phishing: verifique remetente e domínios suspeitos. Reporte mensagens suspeitas \n",
            "ao time de Segurança imediatamente.​\n",
            " \n",
            "4.​ Retenção: mensagens que contenham dados pessoais devem seguir as diretrizes \n",
            "de retenção definidas pela equipe de Privacidade.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'file_path': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Uso de E-mail e Segurança da Informação', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='5.​ Solicitações de liberação de anexos ou domínios devem ser abertas por chamado, \n",
            "com justificativa do gestor.' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'file_path': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Uso de E-mail e Segurança da Informação', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='Política de Reembolsos (Viagens e \n",
            "Despesas) \n",
            " \n",
            "1.​ Reembolso: requer nota fiscal e deve ser submetido em até 10 dias corridos após a \n",
            "despesa.​\n",
            " \n",
            "2.​ Alimentação em viagem: limite de R$ 70/dia por pessoa. Bebidas alcoólicas não \n",
            "são reembolsáveis.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'file_path': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Reembolsos (Viagens e Despesas)', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='são reembolsáveis.​\n",
            " \n",
            "3.​ Transporte: táxi/app são permitidos quando não houver alternativa viável. \n",
            "Comprovantes obrigatórios.​\n",
            " \n",
            "4.​ Internet para home office: reembolsável via subsídio mensal de até R$ 100, \n",
            "conforme política de Home Office.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'file_path': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Reembolsos (Viagens e Despesas)', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='5.​ Cursos e certificações: exigem aprovação prévia do gestor e orçamento do time.​\n",
            " \n",
            "6.​ Custos excepcionais (ex.: franquia de bagagem extra): devem ser justificados no \n",
            "chamado e aprovados antes da compra.' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'file_path': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Reembolsos (Viagens e Despesas)', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='Políticas de Home Office \n",
            " \n",
            "1.​ A empresa adota modelo híbrido: mínimo de 2 dias presenciais por semana, salvo \n",
            "exceções aprovadas pelo gestor e RH.​\n",
            " \n",
            "2.​ Equipamentos: a empresa fornece notebook e periféricos. O colaborador é \n",
            "responsável por zelar pela conservação.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Políticas de Home Office.pdf', 'file_path': '/content/Políticas de Home Office.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Políticas de Home Office', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='3.​ Segurança: é obrigatório uso de VPN e bloqueio de tela. Documentos confidenciais \n",
            "não devem ser impressos fora do escritório.​\n",
            " \n",
            "4.​ Ergonomia: recomendamos cadeira adequada e suporte de monitor. O RH pode \n",
            "avaliar solicitação de apoio ergonômico.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Políticas de Home Office.pdf', 'file_path': '/content/Políticas de Home Office.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Políticas de Home Office', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='5.​ Conectividade: há subsídio mensal de internet domiciliar para quem trabalha em \n",
            "home office: até R$ 100/mês, mediante nota fiscal nominal.​\n",
            " \n",
            "6.​ Solicitação de exceção (ex.: 4-5 dias remotos): deve ser formalizada via chamado \n",
            "ao RH com justificativa do gestor.' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Políticas de Home Office.pdf', 'file_path': '/content/Políticas de Home Office.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Políticas de Home Office', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código itera sobre a lista de pedaços de documentos (chunks) que foram criados na etapa anterior e imprime cada um deles, seguido por uma linha separadora para facilitar a visualização.\n",
        "\n",
        "for chunk in chunks:: Inicia um loop for que irá percorrer cada item na lista chamada chunks. Em cada iteração do loop, o item atual da lista (que é um objeto Document representando um pedaço do texto original) será atribuído à variável chunk.\n",
        "print(chunk): Imprime o conteúdo do objeto chunk atual. Quando você imprime um objeto Document do LangChain, ele geralmente mostra metadados associados ao pedaço (como a fonte ou o número da página) e o conteúdo textual do pedaço.\n",
        "print(\"------------------------------------\"): Imprime uma linha de traços para criar uma separação visual entre a saída de cada pedaço de documento. Isso torna mais fácil ver onde um pedaço termina e o próximo começa na saída do console.\n",
        "Em resumo, este código é uma maneira simples de inspecionar os pedaços de documentos gerados pelo text splitter, permitindo que você veja como os documentos originais foram divididos e qual conteúdo está contido em cada pedaço. Isso é útil para verificar se a divisão foi feita conforme o esperado antes de prosseguir com as próximas etapas do pipeline RAG."
      ],
      "metadata": {
        "id": "qCt20Va1QqT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora realizar uma busca, e dentro desse espaço de vetores baseados nos chunks qual sera a proximidade dos chunks. (Embeddings são representações numéricas de objetos do mundo real que sistemas de aprendizado de máquina (ML) e inteligência artificial (IA) usam para entender domínios de conhecimento complexos como os humanos.Os embeddings representam objetos do mundo real, como palavras, imagens ou vídeos, em uma forma que os computadores podem processar.)"
      ],
      "metadata": {
        "id": "Zr9YHJBpVB8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar biblioteca para embedding"
      ],
      "metadata": {
        "id": "D8f7kRftawVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "_gDKBf5oV1I2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código inicializa um modelo de embeddings do Google Generative AI. Embeddings são representações numéricas de texto que capturam o significado semântico das palavras e frases, o que é essencial para a busca de similaridade em sistemas RAG.\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings: Importa a classe GoogleGenerativeAIEmbeddings do módulo langchain_google_genai. Esta classe permite criar embeddings usando os modelos de embedding do Google.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(...): Cria uma instância da classe GoogleGenerativeAIEmbeddings, configurando o modelo de embedding e a chave de API.\n",
        "model=\"models/gemini-embedding-001\": Especifica qual modelo de embedding do Google será utilizado. gemini-embedding-001 é um modelo de embedding projetado para gerar representações vetoriais de texto.\n",
        "google_api_key=GOOGLE_API_KEY: Fornece a chave de API necessária para autenticar com o serviço de embeddings do Google. A variável GOOGLE_API_KEY é assumida como tendo sido carregada em uma etapa anterior.\n",
        "Em resumo, este código configura o sistema para usar um modelo específico de embedding do Google Generative AI para converter os pedaços de texto (chunks) dos seus documentos em vetores numéricos. Esses vetores serão usados na próxima etapa para criar um índice de busca de similaridade, permitindo que o sistema RAG encontre pedaços de documentos relevantes para a pergunta do usuário."
      ],
      "metadata": {
        "id": "tOREBxvCZ5N2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar biblioteca FAISS para fazer uma busca eficiente e  para fazer o calculo de similaridade."
      ],
      "metadata": {
        "id": "upxQH796cJMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                     search_kwargs={\"score_threshold\":0.3, \"k\": 4})"
      ],
      "metadata": {
        "id": "vUbgGiPdZXDt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código utiliza a biblioteca FAISS para criar um banco de dados vetorial a partir dos pedaços de documentos (chunks) e configura um retriever para buscar informações nesse banco.\n",
        "\n",
        "from langchain_community.vectorstores import FAISS: Importa a classe FAISS do módulo langchain_community.vectorstores. FAISS (Facebook AI Similarity Search) é uma biblioteca para busca de similaridade eficiente, que é ideal para armazenar e consultar vetores de embeddings.\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings): Cria uma instância de um banco de dados vetorial FAISS.\n",
        "FAISS.from_documents(...): Este método constrói o índice FAISS diretamente a partir de uma lista de documentos (neste caso, os chunks) e de um modelo de embeddings (embeddings). Para cada chunk, ele gera um embedding (representação vetorial) usando o modelo especificado e armazena esse vetor no índice FAISS.\n",
        "chunks: A lista de pedaços de documentos gerados na etapa anterior.\n",
        "embeddings: A instância do modelo de embeddings do Google Generative AI configurada anteriormente.\n",
        "retriever = vectorstore.as_retriever(...): Converte o banco de dados vetorial FAISS (vectorstore) em um objeto retriever. Um retriever é um componente do LangChain que sabe como buscar documentos relevantes dado uma consulta.\n",
        "search_type=\"similarity_score_threshold\": Define o tipo de busca a ser realizada pelo retriever. Neste caso, é uma busca por similaridade que só retorna documentos cuja pontuação de similaridade com a consulta esteja acima de um determinado limiar.\n",
        "search_kwargs={\"score_threshold\":0.3, \"k\": 4}: Fornece argumentos adicionais para a busca.\n",
        "\"score_threshold\": 0.3: Define o limiar de pontuação de similaridade. Somente documentos com uma pontuação de similaridade igual ou superior a 0.3 serão retornados. Isso ajuda a filtrar resultados que não são muito relevantes.\n",
        "\"k\": 4: Define o número máximo de documentos mais relevantes a serem retornados (mesmo que haja mais documentos acima do limiar, apenas os 4 mais relevantes serão considerados).\n",
        "Em resumo, este código constrói um índice de busca eficiente dos seus documentos divididos em pedaços, usando os embeddings para representar o significado de cada pedaço. Em seguida, ele configura um retriever que pode ser usado para encontrar os pedaços de documentos mais relevantes para uma determinada pergunta, com base na similaridade semântica e em um limiar de pontuação. Este retriever é um componente chave para a parte de \"Retrieval\" (Recuperação) do sistema RAG."
      ],
      "metadata": {
        "id": "Sfh1xQVNcuwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "criar o prompt par aexplicar o agente como vai funcionar"
      ],
      "metadata": {
        "id": "YrfIbFh_eVcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. \"\n",
        "     \"Responda SOMENTE com base no contexto fornecido. \"\n",
        "     \"Se não houver base suficiente, responda apenas 'Não sei'.\"),\n",
        "\n",
        "    (\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\")\n",
        "])\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag)"
      ],
      "metadata": {
        "id": "axKYyHSWZaWl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código cria um template de prompt para ser usado com um modelo de linguagem em um sistema RAG (Retrieval-Augmented Generation).\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate: Importa a classe ChatPromptTemplate, que permite criar prompts formatados para modelos de chat no LangChain.\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain: Importa a função create_stuff_documents_chain, que é usada para criar uma cadeia que pega uma lista de documentos e os \"junta\" (stuff) em um único prompt para o modelo de linguagem.\n",
        "prompt_rag = ChatPromptTemplate.from_messages([...]): Cria uma instância de ChatPromptTemplate chamada prompt_rag. O método from_messages permite definir o prompt usando uma lista de mensagens, que é um formato comum para interagir com modelos de chat.\n",
        "(\"system\", \"Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. Responda SOMENTE com base no contexto fornecido. Se não houver base suficiente, responda apenas 'Não sei'.\"): Define uma mensagem do sistema. Esta mensagem estabelece o papel do modelo (Assistente de Políticas Internas), instrui-o a responder apenas com base no contexto fornecido e a dizer \"Não sei\" se o contexto for insuficiente.\n",
        "(\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\"): Define uma mensagem do usuário (humana). Esta é a estrutura do prompt que será enviado ao modelo. Ele inclui um placeholder {input} para a pergunta do usuário e um placeholder {context} onde os pedaços relevantes dos documentos recuperados serão inseridos.\n",
        "document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag): Cria uma cadeia de documentos usando a função create_stuff_documents_chain.\n",
        "llm_triagem: A instância do modelo de linguagem (neste caso, gemini-2.5-flash) que foi configurada anteriormente.\n",
        "prompt_rag: O template de prompt que acabamos de definir.\n",
        "Essa cadeia (document_chain) receberá a pergunta do usuário e os documentos relevantes (recuperados pelo retriever), formatará tudo de acordo com o prompt_rag e enviará para o llm_triagem para gerar a resposta final.\n",
        "\n",
        "Em resumo, este código prepara o template de prompt que o modelo de linguagem usará para responder às perguntas do usuário, incorporando o contexto recuperado dos documentos, e cria uma cadeia que orquestra a passagem desses dados para o modelo."
      ],
      "metadata": {
        "id": "Bg8Nx58ie_rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auxilia formatar o texto"
      ],
      "metadata": {
        "id": "Jis2AU5Eh3lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatadores\n",
        "import re, pathlib\n",
        "\n",
        "def _clean_text(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "\n",
        "def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:\n",
        "    txt = _clean_text(texto)\n",
        "    termos = [t.lower() for t in re.findall(r\"\\w+\", query or \"\") if len(t) >= 4]\n",
        "    pos = -1\n",
        "    for t in termos:\n",
        "        pos = txt.lower().find(t)\n",
        "        if pos != -1: break\n",
        "    if pos == -1: pos = 0\n",
        "    ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2)\n",
        "    return txt[ini:fim]\n",
        "\n",
        "def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:\n",
        "    cites, seen = [], set()\n",
        "    for d in docs_rel:\n",
        "        src = pathlib.Path(d.metadata.get(\"source\",\"\")).name\n",
        "        page = int(d.metadata.get(\"page\", 0)) + 1\n",
        "        key = (src, page)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        cites.append({\"documento\": src, \"pagina\": page, \"trecho\": extrair_trecho(d.page_content, query)})\n",
        "    return cites[:3]"
      ],
      "metadata": {
        "id": "FTJW2qHdZd2u"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Este trecho de código contém funções auxiliares para formatar o texto e extrair trechos relevantes dos documentos para uso nas citações do sistema RAG.\n",
        "\n",
        "import re, pathlib: Importa os módulos re (para expressões regulares) e pathlib (para manipulação de caminhos de arquivo orientada a objetos).\n",
        "def _clean_text(s: str) -> str:: Define uma função privada (_ no início indica que é para uso interno) chamada _clean_text. Ela recebe uma string s e retorna uma string.\n",
        "return re.sub(r\"\\s+\", \" \", s or \"\").strip(): Esta linha limpa o texto. s or \"\" garante que se s for None ou vazio, uma string vazia seja usada. re.sub(r\"\\s+\", \" \", ...) substitui uma ou mais ocorrências de espaços em branco (incluindo quebras de linha, tabs, etc.) por um único espaço. .strip() remove espaços em branco do início e do fim da string resultante.\n",
        "def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:: Define uma função chamada extrair_trecho que recebe o texto do documento, a query (pergunta) do usuário e um tamanho de janela (padrão de 240 caracteres) e retorna uma string. O objetivo é encontrar um trecho do texto que contenha termos da consulta.\n",
        "txt = _clean_text(texto): Limpa o texto do documento usando a função auxiliar _clean_text.\n",
        "termos = [t.lower() for t in re.findall(r\"\\w+\", query or \"\") if len(t) >= 4]: Extrai palavras da query com 4 ou mais caracteres e as converte para minúsculas. Essas são as palavras-chave que a função tentará encontrar no texto do documento.\n",
        "pos = -1: Inicializa a posição encontrada como -1.\n",
        "for t in termos: ... if pos != -1: break: Itera sobre os termos da consulta. Para cada termo, ele procura a primeira ocorrência (ignorando maiúsculas/minúsculas) no texto limpo. Se encontrar, a posição é armazenada em pos e o loop é interrompido.\n",
        "if pos == -1: pos = 0: Se nenhum termo da consulta for encontrado no texto, define a posição como 0 (o início do texto).\n",
        "ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2): Calcula as posições de início (ini) e fim (fim) do trecho a ser extraído. Ele tenta centralizar a janela em torno da pos onde um termo foi encontrado (ou no início se nenhum termo foi encontrado), garantindo que as posições estejam dentro dos limites do texto.\n",
        "return txt[ini:fim]: Retorna o trecho do texto que vai da posição ini até a posição fim.\n",
        "def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:: Define uma função chamada formatar_citacoes que recebe uma lista de documentos relevantes (docs_rel) e a query original, e retorna uma lista de dicionários. O objetivo é gerar uma lista formatada de citações dos documentos recuperados.\n",
        "cites, seen = [], set(): Inicializa uma lista vazia cites para armazenar as citações formatadas e um conjunto vazio seen para rastrear documentos/páginas já citados (evitando duplicatas).\n",
        "for d in docs_rel:: Itera sobre cada documento (d) na lista de documentos relevantes.\n",
        "src = pathlib.Path(d.metadata.get(\"source\",\"\")).name: Extrai o nome do arquivo fonte dos metadados do documento.\n",
        "page = int(d.metadata.get(\"page\", 0)) + 1: Extrai o número da página dos metadados do documento e adiciona 1 (já que geralmente as páginas são indexadas a partir de 0).\n",
        "key = (src, page): Cria uma chave única combinando o nome do arquivo e o número da página.\n",
        "if key in seen: continue: Verifica se esta combinação de arquivo e página já foi citada. Se sim, pula para a próxima iteração (evita duplicatas).\n",
        "seen.add(key): Adiciona a chave ao conjunto seen para marcar este documento/página como citada.\n",
        "cites.append({\"documento\": src, \"pagina\": page, \"trecho\": extrair_trecho(d.page_content, query)}): Cria um dicionário para a citação contendo o nome do documento, a página e um trecho relevante extraído do conteúdo do documento (d.page_content) usando a função extrair_trecho. Este dicionário é então adicionado à lista cites.\n",
        "return cites[:3]: Retorna apenas as 3 primeiras citações encontradas.\n",
        "Em resumo, este código fornece as ferramentas para limpar texto, extrair trechos relevantes de um documento com base em uma consulta e formatar uma lista de citações únicas com base nos documentos recuperados e a consulta original, limitando o número de citações retornadas.\n",
        "\n"
      ],
      "metadata": {
        "id": "KMMKM5fumDPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar a função que vai fazer as perguntas (principal)"
      ],
      "metadata": {
        "id": "L_wh-heuicsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perguntar_politica_RAG(pergunta: str) -> Dict:\n",
        "    docs_relacionados = retriever.invoke(pergunta)\n",
        "\n",
        "    if not docs_relacionados:\n",
        "        return {\"answer\": \"Não sei.\",\n",
        "                \"citacoes\": [],\n",
        "                \"contexto_encontrado\": False}\n",
        "\n",
        "    answer = document_chain.invoke({\"input\": pergunta,\n",
        "                                    \"context\": docs_relacionados})\n",
        "\n",
        "    txt = (answer or \"\").strip()\n",
        "\n",
        "    if txt.rstrip(\".!?\") == \"Não sei\":\n",
        "        return {\"answer\": \"Não sei.\",\n",
        "                \"citacoes\": [],\n",
        "                \"contexto_encontrado\": False}\n",
        "\n",
        "    return {\"answer\": txt,\n",
        "            \"citacoes\": formatar_citacoes(docs_relacionados, pergunta),\n",
        "            \"contexto_encontrado\": True}"
      ],
      "metadata": {
        "id": "qwrn3YRvZiIl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código define a função principal perguntar_politica_RAG que orquestra o processo de resposta a uma pergunta do usuário usando o sistema RAG que você construiu.\n",
        "\n",
        "def perguntar_politica_RAG(pergunta: str) -> Dict:: Define uma função chamada perguntar_politica_RAG que aceita uma string (pergunta) como entrada e é anotada para retornar um dicionário (Dict).\n",
        "docs_relacionados = retriever.invoke(pergunta): Esta linha é a parte de \"Retrieval\" (Recuperação) do RAG. Ela usa o objeto retriever (que foi configurado para buscar no banco de dados vetorial FAISS) para encontrar os documentos (chunks) mais relevantes para a pergunta fornecida pelo usuário. O resultado é armazenado na variável docs_relacionados.\n",
        "if not docs_relacionados:: Verifica se o retriever retornou algum documento relacionado. Se a lista docs_relacionados estiver vazia (ou seja, nenhum documento relevante foi encontrado com base no limiar de similaridade), o sistema não tem informações para responder.\n",
        "return {\"answer\": \"Não sei.\", \"citacoes\": [], \"contexto_encontrado\": False}: Se nenhum documento relacionado for encontrado, a função retorna um dicionário indicando que a resposta é \"Não sei\", uma lista vazia de citações e contexto_encontrado como False.\n",
        "answer = document_chain.invoke({\"input\": pergunta, \"context\": docs_relacionados}): Se documentos relacionados forem encontrados, esta linha executa a parte de \"Generation\" (Geração) do RAG. Ela invoca a document_chain (que combina a pergunta do usuário e os documentos recuperados no prompt e envia para o modelo de linguagem). O dicionário passado como argumento para invoke contém a pergunta do usuário (input) e a lista de docs_relacionados (context). A resposta gerada pelo modelo é armazenada na variável answer.\n",
        "txt = (answer or \"\").strip(): Pega o conteúdo da resposta (answer), converte para string caso não seja e remove espaços em branco do início e do fim.\n",
        "if txt.rstrip(\".!?\") == \"Não sei\":: Verifica se a resposta gerada pelo modelo é \"Não sei\" (removendo pontuações finais como ., ! ou ? para garantir a comparação). Lembre-se que o prompt do sistema instrui o modelo a responder \"Não sei\" se não tiver base suficiente.\n",
        "return {\"answer\": \"Não sei.\", \"citacoes\": [], \"contexto_encontrado\": False}: Se a resposta do modelo for \"Não sei\", a função retorna o mesmo dicionário de \"Não sei\" retornado quando nenhum documento foi encontrado.\n",
        "return {\"answer\": txt, \"citacoes\": formatar_citacoes(docs_relacionados, pergunta), \"contexto_encontrado\": True}: Se o modelo gerou uma resposta diferente de \"Não sei\" (o que implica que ele usou o contexto fornecido), a função retorna um dicionário contendo:\n",
        "\"answer\": A resposta gerada pelo modelo (txt).\n",
        "\"citacoes\": Uma lista de citações formatadas, geradas pela função formatar_citacoes (definida em outra célula) com base nos docs_relacionados e na pergunta original. Isso ajuda a dar transparência sobre de onde a resposta veio.\n",
        "\"contexto_encontrado\": True, indicando que documentos relevantes foram encontrados e usados para gerar a resposta.\n",
        "Em resumo, a função perguntar_politica_RAG é o coração do seu sistema RAG. Ela recebe uma pergunta, busca documentos relevantes na sua base de conhecimento, usa esses documentos como contexto para o modelo de linguagem gerar uma resposta e formata a saída, incluindo citações, se um contexto relevante foi encontrado."
      ],
      "metadata": {
        "id": "f8tDceb-jth1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "xsMGEdDMZlol"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "    resposta = perguntar_politica_RAG(msg_teste)\n",
        "    print(f\"PERGUNTA: {msg_teste}\")\n",
        "    print(f\"RESPOSTA: {resposta['answer']}\")\n",
        "    if resposta['contexto_encontrado']:\n",
        "        print(\"CITAÇÕES:\")\n",
        "        for c in resposta['citacoes']:\n",
        "            print(f\" - Documento: {c['documento']}, Página: {c['pagina']}\")\n",
        "            print(f\"   Trecho: {c['trecho']}\")\n",
        "        print(\"------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lxT7ELFZpxl",
        "outputId": "98facb32-cd78-4e60-b612-d1baa5441f31"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERGUNTA: Posso reembolsar a internet?\n",
            "RESPOSTA: Sim, a internet para home office é reembolsável via subsídio mensal de até R$ 100, mediante nota fiscal nominal.\n",
            "CITAÇÕES:\n",
            " - Documento: Política de Reembolsos (Viagens e Despesas).pdf, Página: 1\n",
            "   Trecho: lsáveis.​ 3.​ Transporte: táxi/app são permitidos quando não houver alternativa viável. Comprovantes obrigatórios.​ 4.​ Internet para home office: reembolsável via subsídio mensal de até R$ 100, conforme política de Home Office.​\n",
            " - Documento: Políticas de Home Office.pdf, Página: 1\n",
            "   Trecho: 5.​ Conectividade: há subsídio mensal de internet domiciliar para quem trabalha em home office: até R$ 100/mês, mediante nota fiscal nominal.​ 6.​ Solicitação de\n",
            "------------------------------------\n",
            "PERGUNTA: Quero mais 5 dias de trabalho remoto. Como faço?\n",
            "RESPOSTA: Para solicitar mais 5 dias de trabalho remoto, você deve formalizar a solicitação via chamado ao RH com a justificativa do seu gestor. Lembre-se que o modelo padrão da empresa é híbrido, com mínimo de 2 dias presenciais por semana, e sua solicitação será tratada como uma exceção.\n",
            "CITAÇÕES:\n",
            " - Documento: Políticas de Home Office.pdf, Página: 1\n",
            "   Trecho:  para quem trabalha em home office: até R$ 100/mês, mediante nota fiscal nominal.​ 6.​ Solicitação de exceção (ex.: 4-5 dias remotos): deve ser formalizada via chamado ao RH com justificativa do gestor.\n",
            " - Documento: Política de Reembolsos (Viagens e Despesas).pdf, Página: 1\n",
            "   Trecho: são reembolsáveis.​ 3.​ Transporte: táxi/app são permitidos quando não houver alternativa viável. Comprovantes obrigatór\n",
            "------------------------------------\n",
            "PERGUNTA: Posso reembolsar cursos ou treinamentos da Alura?\n",
            "RESPOSTA: Cursos e certificações são reembolsáveis, desde que haja aprovação prévia do gestor e orçamento do time.\n",
            "CITAÇÕES:\n",
            " - Documento: Política de Reembolsos (Viagens e Despesas).pdf, Página: 1\n",
            "   Trecho: Política de Reembolsos (Viagens e Despesas) 1.​ Reembolso: requer nota fiscal e deve ser submetido em até 10 dias corrid\n",
            " - Documento: Políticas de Home Office.pdf, Página: 1\n",
            "   Trecho: Políticas de Home Office 1.​ A empresa adota modelo híbrido: mínimo de 2 dias presenciais por semana, salvo exceções apr\n",
            "------------------------------------\n",
            "PERGUNTA: Quantas capivaras tem no Rio Pinheiros?\n",
            "RESPOSTA: Não sei.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código itera sobre uma lista de mensagens de teste e utiliza a função perguntar_politica_RAG para obter a resposta do sistema RAG para cada pergunta. Em seguida, ele imprime a pergunta original, a resposta gerada pelo sistema e, se houver contexto relevante encontrado, imprime as citações de onde a resposta foi extraída.\n",
        "\n",
        "for msg_teste in testes:: Inicia um loop for que irá percorrer cada item na lista chamada testes. Em cada iteração do loop, o item atual da lista será atribuído à variável msg_teste.\n",
        "resposta = perguntar_politica_RAG(msg_teste): Chama a função perguntar_politica_RAG (que definimos anteriormente) passando a mensagem de teste atual (msg_teste) como argumento. Esta função executa todo o processo de RAG (recuperação de documentos relevantes, envio para o modelo de linguagem com contexto) e retorna um dicionário contendo a resposta, as citações e um indicador se o contexto foi encontrado. O resultado é armazenado na variável resposta.\n",
        "print(f\"PERGUNTA: {msg_teste}\"): Imprime a pergunta de teste atual.\n",
        "print(f\"RESPOSTA: {resposta['answer']}\"): Imprime a resposta gerada pelo sistema RAG, acessando o valor associado à chave 'answer' no dicionário resposta.\n",
        "if resposta['contexto_encontrado']:: Verifica se o valor associado à chave 'contexto_encontrado' no dicionário resposta é True. Isso indica que o sistema encontrou documentos relevantes e usou-os como contexto para gerar a resposta.\n",
        "print(\"CITAÇÕES:\"): Se contexto_encontrado for True, imprime o cabeçalho \"CITAÇÕES:\".\n",
        "for c in resposta['citacoes']:: Inicia um loop aninhado que itera sobre a lista de citações armazenada na chave 'citacoes' do dicionário resposta. Cada item nesta lista (c) é um dicionário contendo informações sobre a citação.\n",
        "print(f\" - Documento: {c['documento']}, Página: {c['pagina']}\"): Dentro do loop de citações, imprime o nome do documento e o número da página de onde o trecho foi extraído.\n",
        "print(f\" Trecho: {c['trecho']}\"): Imprime o trecho específico do documento que foi recuperado como parte da citação.\n",
        "print(\"------------------------------------\"): Imprime uma linha separadora após as citações de cada pergunta (se houver) para melhorar a legibilidade da saída.\n",
        "Em resumo, este código demonstra como usar a função perguntar_politica_RAG com uma lista de perguntas de teste e exibe os resultados de forma estruturada, incluindo as citações relevantes quando o sistema consegue encontrar informações na base de conhecimento."
      ],
      "metadata": {
        "id": "5PUmNSadk0lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AULA 03 = ORQUESTRAÇÃO COM AGENTE LANGGRAPH"
      ],
      "metadata": {
        "id": "RBmT64D21aZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "iNSTALAR o Langgraph"
      ],
      "metadata": {
        "id": "lES7_cHt1f6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWI08ZjpZtRN",
        "outputId": "2325c67a-e43d-4e47-b746-aecb46fa2b1b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/153.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como montar a logicas dos grafos,  quem é o agente"
      ],
      "metadata": {
        "id": "3QakDmj52WIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Optional\n",
        "\n",
        "class AgentState(TypedDict, total = False):\n",
        "    pergunta: str\n",
        "    triagem: dict\n",
        "    resposta: Optional[str]\n",
        "    citacoes: List[dict]\n",
        "    rag_sucesso: bool\n",
        "    acao_final: str"
      ],
      "metadata": {
        "id": "mO1m6w6c18ek"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código define uma estrutura de dados chamada AgentState usando TypedDict do módulo typing. Essa estrutura é usada para representar o estado de um agente em um workflow, como os construídos com a biblioteca langgraph.\n",
        "\n",
        "from typing import TypedDict, Optional, List: Importa os tipos TypedDict, Optional e List do módulo typing.\n",
        "\n",
        "TypedDict: Permite definir um tipo de dicionário onde você especifica os nomes e os tipos dos campos esperados. Isso fornece tipagem estática e clareza sobre a estrutura dos dados.\n",
        "\n",
        "Optional: Indica que um campo pode ter o tipo especificado ou ser None.\n",
        "\n",
        "List: Indica que um campo é uma lista de itens de um tipo especificado.\n",
        "\n",
        "class AgentState(TypedDict, total = False):: Define uma nova classe AgentState que herda de TypedDict.\n",
        "\n",
        "total = False: Este argumento opcional significa que nem todos os campos listados na definição da classe são obrigatórios. Um dicionário do tipo AgentState pode não conter todos esses campos.\n",
        "\n",
        "pergunta: str: Define um campo chamado pergunta que deve ser uma string (str).\n",
        "triagem: dict: Define um campo chamado triagem que deve ser um dicionário (dict).\n",
        "\n",
        "resposta: Optional[str]: Define um campo chamado resposta que pode ser uma string (str) ou None.\n",
        "\n",
        "citacoes: List[dict]: Define um campo chamado citacoes que deve ser uma lista (List) de dicionários (dict).\n",
        "\n",
        "rag_sucesso: bool: Define um campo chamado rag_sucesso que deve ser um booleano (bool), indicando se o processo RAG foi bem-sucedido.\n",
        "\n",
        "acao_final: str: Define um campo chamado acao_final que deve ser uma string (str), representando a ação final tomada pelo agente.\n",
        "\n",
        "Em resumo, esta classe AgentState define um esquema para um dicionário que manterá o estado durante a execução de um agente. Ela especifica que o estado conterá a pergunta original, o resultado da triagem, a resposta gerada pelo RAG (opcional), as citações relevantes, um indicador de sucesso do RAG e a ação final decidida. O uso de TypedDict ajuda a garantir que os dados passados entre as diferentes etapas do agente sigam uma estrutura consistente."
      ],
      "metadata": {
        "id": "jtLITev_2DEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "criar os codigos a partir do diagrama criado ![ALURA02_LANGGRAPH.png]()"
      ],
      "metadata": {
        "id": "FJnKuPmb9TY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_triagem(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de triagem...\")\n",
        "    return {\"triagem\": triagem(state[\"pergunta\"])}"
      ],
      "metadata": {
        "id": "sog5D1iD8pOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_auto_resolver(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de auto_resolver...\")\n",
        "    resposta_rag = perguntar_politica_RAG(state[\"pergunta\"])\n",
        "\n",
        "    update: AgentState = {\n",
        "        \"resposta\": resposta_rag[\"answer\"],\n",
        "        \"citacoes\": resposta_rag.get(\"citacoes\", []),\n",
        "        \"rag_sucesso\": resposta_rag[\"contexto_encontrado\"],\n",
        "    }\n",
        "\n",
        "    if resposta_rag[\"contexto_encontrado\"]:\n",
        "        update[\"acao_final\"] = \"AUTO_RESOLVER\"\n",
        "\n",
        "    return update"
      ],
      "metadata": {
        "id": "MyFZOyQY8qDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_pedir_info(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de pedir_info...\")\n",
        "    faltantes = state[\"triagem\"].get(\"campos_faltantes\", [])\n",
        "    if faltantes:\n",
        "        detalhe = \",\".join(faltantes)\n",
        "    else:\n",
        "        detalhe = \"Tema e contexto específico\"\n",
        "\n",
        "    return {\n",
        "        \"resposta\": f\"Para avançar, preciso que detalhe: {detalhe}\",\n",
        "        \"citacoes\": [],\n",
        "        \"acao_final\": \"PEDIR_INFO\"\n",
        "    }"
      ],
      "metadata": {
        "id": "fytM9hmt8s-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_abrir_chamado(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de abrir_chamado...\")\n",
        "    triagem = state[\"triagem\"]\n",
        "\n",
        "    return {\n",
        "        \"resposta\": f\"Abrindo chamado com urgência {triagem['urgencia']}. Descrição: {state['pergunta'][:140]}\",\n",
        "        \"citacoes\": [],\n",
        "        \"acao_final\": \"ABRIR_CHAMADO\"\n",
        "    }"
      ],
      "metadata": {
        "id": "Bd2yndsC8wWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORDS_ABRIR_TICKET = [\"aprovação\", \"exceção\", \"liberação\", \"abrir ticket\", \"abrir chamado\", \"acesso especial\"]\n",
        "\n",
        "def decidir_pos_triagem(state: AgentState) -> str:\n",
        "    print(\"Decidindo após a triagem...\")\n",
        "    decisao = state[\"triagem\"][\"decisao\"]\n",
        "\n",
        "    if decisao == \"AUTO_RESOLVER\": return \"auto\"\n",
        "    if decisao == \"PEDIR_INFO\": return \"info\"\n",
        "    if decisao == \"ABRIR_CHAMADO\": return \"chamado\""
      ],
      "metadata": {
        "id": "2NrhvMBw8z2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decidir_pos_auto_resolver(state: AgentState) -> str:\n",
        "    print(\"Decidindo após o auto_resolver...\")\n",
        "\n",
        "    if state.get(\"rag_sucesso\"):\n",
        "        print(\"Rag com sucesso, finalizando o fluxo.\")\n",
        "        return \"ok\"\n",
        "\n",
        "    state_da_pergunta = (state[\"pergunta\"] or \"\").lower()\n",
        "\n",
        "    if any(k in state_da_pergunta for k in KEYWORDS_ABRIR_TICKET):\n",
        "        print(\"Rag falhou, mas foram encontradas keywords de abertura de ticket. Abrindo...\")\n",
        "        return \"chamado\"\n",
        "\n",
        "    print(\"Rag falhou, sem keywords, vou pedir mais informações...\")\n",
        "    return \"info\""
      ],
      "metadata": {
        "id": "rNNfiQNG83R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"triagem\", node_triagem)\n",
        "workflow.add_node(\"auto_resolver\", node_auto_resolver)\n",
        "workflow.add_node(\"pedir_info\", node_pedir_info)\n",
        "workflow.add_node(\"abrir_chamado\", node_abrir_chamado)\n",
        "\n",
        "workflow.add_edge(START, \"triagem\")\n",
        "workflow.add_conditional_edges(\"triagem\", decidir_pos_triagem, {\n",
        "    \"auto\": \"auto_resolver\",\n",
        "    \"info\": \"pedir_info\",\n",
        "    \"chamado\": \"abrir_chamado\"\n",
        "})\n",
        "\n",
        "workflow.add_conditional_edges(\"auto_resolver\", decidir_pos_auto_resolver, {\n",
        "    \"info\": \"pedir_info\",\n",
        "    \"chamado\": \"abrir_chamado\",\n",
        "    \"ok\": END\n",
        "})\n",
        "\n",
        "workflow.add_edge(\"pedir_info\", END)\n",
        "workflow.add_edge(\"abrir_chamado\", END)\n",
        "\n",
        "grafo = workflow.compile()"
      ],
      "metadata": {
        "id": "-G16lnoP861U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rom IPython.display import display, Image\n",
        "\n",
        "graph_bytes = grafo.get_graph().draw_mermaid_png()\n",
        "display(Image(graph_bytes))"
      ],
      "metadata": {
        "id": "Kdzs_7fU8_zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"É possível reembolsar certificações do Google Cloud?\",\n",
        "          \"Posso obter o Google Gemini de graça?\",\n",
        "          \"Qual é a palavra-chave da aula de hoje?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "Hek_lWVm9HUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_test in testes:\n",
        "    resposta_final = grafo.invoke({\"pergunta\": msg_test})\n",
        "\n",
        "    triag = resposta_final.get(\"triagem\", {})\n",
        "    print(f\"PERGUNTA: {msg_test}\")\n",
        "    print(f\"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}\")\n",
        "    print(f\"RESPOSTA: {resposta_final.get('resposta')}\")\n",
        "    if resposta_final.get(\"citacoes\"):\n",
        "        print(\"CITAÇÕES:\")\n",
        "        for citacao in resposta_final.get(\"citacoes\"):\n",
        "            print(f\" - Documento: {citacao['documento']}, Página: {citacao['pagina']}\")\n",
        "            print(f\"   Trecho: {citacao['trecho']}\")\n",
        "\n",
        "    print(\"------------------------------------\")"
      ],
      "metadata": {
        "id": "eLp-lqLC9INL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}