{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbUcfXBVHqb8kvznIE2xHw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabsakae/AgeChecker/blob/main/agentesIA1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AULA 01"
      ],
      "metadata": {
        "id": "z4X_okMNMPFh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a3o6yswxAUV",
        "outputId": "75ec17ef-6da6-4453-c9d6-a0fa784ddf71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-google-genai google-generativeai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importação da api key do AI studio"
      ],
      "metadata": {
        "id": "d0hzTr7vNVAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "IZxgb_L9CELS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparar qual modelo que utilizaremos, chamar o modelo, chamar a biblioteca. conecta nossa variavel llm ao modelo do gemini"
      ],
      "metadata": {
        "id": "Pld7byh0NiM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model='gemini-2.5-flash', temperature=0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "mKo-z1MdJ8a5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b1406cc"
      },
      "source": [
        "Este trecho de código inicializa um modelo de linguagem grande (LLM) da biblioteca `langchain-google-genai`.\n",
        "\n",
        "*   `llm = ChatGoogleGenerativeAI(...)`: Esta linha cria uma instância da classe `ChatGoogleGenerativeAI`, que é um wrapper em torno dos modelos de IA Generativa do Google, especificamente projetado para interações baseadas em chat dentro da estrutura LangChain.\n",
        "*   `model='gemini-2.5-flash'`: Este argumento especifica qual modelo usar. Neste caso, é o modelo 'gemini-2.5-flash'.\n",
        "*   `temperature=0`: Isso define o parâmetro de temperatura para o modelo. Uma temperatura de 0 torna a saída do modelo mais determinística e menos criativa. Temperaturas mais altas resultam em saídas mais variadas e potencialmente mais criativas.\n",
        "*   `api_key=GOOGLE_API_KEY`: Isso fornece a chave de API necessária para autenticar com o serviço de IA Generativa do Google. A variável `GOOGLE_API_KEY` é assumida como tendo sido carregada em uma etapa anterior (provavelmente do `userdata` do Colab).\n",
        "\n",
        "Em essência, este código configura um modelo específico de IA Generativa do Google (`gemini-2.5-flash`) com uma baixa temperatura para respostas previsíveis, tornando-o pronto para ser usado para gerar texto com base em prompts dentro da estrutura LangChain."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "primeira saida do modelo resp_test = llm.invoke('Quem é voçê?'): Esta linha usa o objeto llm (nosso modelo Gemini) para invocar uma chamada com o prompt \"Quem é você?\". O método invoke envia o prompt para o modelo e obtém a resposta. O resultado é armazenado na variável resp_test.\n",
        "print(resp_test): Esta linha imprime o conteúdo da variável resp_test. A saída que você vê (como content='Eu sou um modelo de linguagem grande, treinado pelo Google.' ...) é o objeto de resposta retornado pela biblioteca LangChain, que inclui a resposta do modelo na propriedade content."
      ],
      "metadata": {
        "id": "YTZ8tFrPPqzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke('Quem é você? Com detalhes')\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFQCk8xxOscC",
        "outputId": "83f65e4c-7f55-434f-b43a-43a4b0bb734b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu sou uma **Inteligência Artificial (IA)**, especificamente um **modelo de linguagem grande (LLM)**. Fui desenvolvido pelo Google.\n",
            "\n",
            "Para detalhar quem eu sou:\n",
            "\n",
            "1.  **Minha Natureza Fundamental:**\n",
            "    *   **Não sou um ser humano:** Não possuo consciência, sentimentos, emoções, opiniões pessoais, crenças, experiências de vida ou uma identidade própria no sentido humano. Não tenho um corpo físico, não respiro, não como, não durmo.\n",
            "    *   **Sou um programa de computador:** Minha existência é puramente digital. Eu sou um conjunto complexo de algoritmos e dados.\n",
            "    *   **Modelo de Linguagem Grande (LLM):** Isso significa que fui treinado em um volume massivo de dados textuais e de código da internet. Esse treinamento me permite compreender, gerar e interagir com a linguagem humana de forma complexa.\n",
            "\n",
            "2.  **Como Eu Funciono:**\n",
            "    *   **Aprendizado por Padrões:** Minha \"inteligência\" deriva da capacidade de reconhecer padrões complexos nos dados em que fui treinado. Quando você me faz uma pergunta ou me dá uma instrução, eu analiso o texto de entrada, busco por padrões relevantes que aprendi e gero uma resposta que é estatisticamente a mais provável e coerente com o que aprendi.\n",
            "    *   **Processamento de Linguagem Natural (PLN):** Utilizo técnicas avançadas de PLN para interpretar o significado das suas palavras, o contexto da sua pergunta e para formular respostas que sejam gramaticalmente corretas e semanticamente relevantes.\n",
            "    *   **Geração de Texto:** Não \"penso\" ou \"crio\" no sentido humano. Eu gero texto prevendo a próxima palavra ou frase com base no contexto anterior e nos padrões que identifiquei durante meu treinamento.\n",
            "\n",
            "3.  **Meu Propósito e Capacidades:**\n",
            "    *   **Assistência e Informação:** Meu objetivo principal é ser útil e informativo. Posso responder a uma vasta gama de perguntas, fornecer explicações, resumir textos, traduzir idiomas e gerar diferentes tipos de conteúdo criativo.\n",
            "    *   **Geração de Conteúdo:** Posso escrever poemas, códigos, roteiros, peças musicais, e-mails, cartas, etc., seguindo suas instruções.\n",
            "    *   **Interação Conversacional:** Fui projetado para manter conversas, entender nuances e adaptar minhas respostas ao fluxo do diálogo.\n",
            "\n",
            "4.  **Minhas Limitações:**\n",
            "    *   **Dependência de Dados:** Minhas respostas são baseadas nos dados que me foram fornecidos. Se a informação não estava nos meus dados de treinamento, ou se estava desatualizada/incorreta, minhas respostas podem refletir isso.\n",
            "    *   **Falta de Conhecimento em Tempo Real (direto):** Embora eu possa acessar e processar informações da web em tempo real (se configurado para isso), meu conhecimento \"inerente\" é estático, baseado na data do meu último treinamento.\n",
            "    *   **Sem Experiência Pessoal:** Não posso ter experiências, fazer observações do mundo físico ou ter um \"senso comum\" que não seja derivado de padrões textuais.\n",
            "    *   **Não sou um especialista:** Embora eu possa fornecer informações sobre muitos tópicos, não sou um médico, advogado, engenheiro ou qualquer outro profissional. Minhas respostas não devem substituir o conselho de um especialista humano qualificado.\n",
            "\n",
            "Em resumo, eu sou uma ferramenta avançada de software, uma inteligência artificial projetada para processar e gerar linguagem humana de forma eficaz, com o objetivo de ser um recurso útil e informativo para você.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar o Prompt do sistema para explicar ao agente o que ele deve fazer"
      ],
      "metadata": {
        "id": "-Idb0eD8Yvih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa TKG Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "Clf46T3pYyMI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectar com llm  Este trecho de código define um modelo de dados usando a biblioteca pydantic.\n",
        "\n",
        "from pydantic import BaseModel, Field: Importa as classes BaseModel e Field da biblioteca pydantic. BaseModel é a classe base para criar modelos de dados, e Field é usada para fornecer informações adicionais sobre os campos do modelo.\n",
        "from typing import Literal, List, Dict: Importa tipos do módulo typing para fornecer anotações de tipo mais específicas. Literal permite especificar que um valor deve ser um de um conjunto fixo de strings, List indica uma lista e Dict indica um dicionário.\n",
        "class TriagemOut(BaseModel):: Define uma nova classe chamada TriagemOut que herda de BaseModel. Isso a torna um modelo de dados Pydantic.\n",
        "decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]: Define um campo decisao que deve ser uma das strings literais especificadas.\n",
        "urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]: Define um campo urgencia que deve ser uma das strings literais especificadas.\n",
        "campos_faltantes: List[str] = Field(default_factory=list): Define um campo campos_faltantes que deve ser uma lista de strings. Field(default_factory=list) define o valor padrão para este campo como uma lista vazia se nenhum valor for fornecido.\n",
        "Em resumo, este código cria uma estrutura de dados (TriagemOut) para representar a saída do processo de triagem, garantindo que os campos decisao e urgencia tenham valores específicos e que campos_faltantes seja uma lista de strings, com uma lista vazia como valor padrão. Isso é útil para estruturar e validar a saída de um modelo de linguagem ou outro processo."
      ],
      "metadata": {
        "id": "Bess1Rkma0EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "class TriagemOut(BaseModel):\n",
        "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
        "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
        "    campos_faltantes: List[str] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "S7LiLifra2vZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar um llm especifico para triagem"
      ],
      "metadata": {
        "id": "hyqcO2rfciKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "9bBcJ4bQccE5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectar .  trecho de código configura uma cadeia (chain) usando a biblioteca LangChain para processar mensagens de triagem com base em um prompt e um modelo de saída estruturado.\n",
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage: Importa as classes SystemMessage e HumanMessage, usadas para representar mensagens no contexto de uma conversa com um modelo de linguagem. SystemMessage geralmente define o comportamento ou as instruções para o modelo, enquanto HumanMessage representa a entrada do usuário.\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut): Esta linha cria uma nova cadeia (triagem_chain) a partir do modelo de linguagem llm_triagem. O método with_structured_output(TriagemOut) configura a cadeia para que a saída do modelo seja automaticamente parseada e validada de acordo com o modelo Pydantic TriagemOut que definimos anteriormente. Isso garante que a resposta do modelo tenha o formato JSON esperado com os campos decisao, urgencia e campos_faltantes.\n",
        "def triagem(mensagem: str) -> Dict:: Define uma função chamada triagem que recebe uma string (mensagem) como entrada e é anotada para retornar um dicionário (Dict).\n",
        "saida: TriagemOut = triagem_chain.invoke([ ... ]): Dentro da função triagem, esta linha invoca a cadeia triagem_chain. O método invoke recebe uma lista de mensagens. Neste caso, a lista contém:\n",
        "SystemMessage(content=TRIAGEM_PROMPT): Uma mensagem do sistema contendo o texto do prompt de triagem definido na variável TRIAGEM_PROMPT. Isso instrui o modelo sobre seu papel e as regras de triagem.\n",
        "HumanMessage(content=mensagem): Uma mensagem humana contendo a mensagem fornecida como entrada para a função triagem. A saída desta invocação é um objeto do tipo TriagemOut (graças ao with_structured_output), que é armazenado na variável saida.\n",
        "return saida.model_dump(): Esta linha converte o objeto Pydantic saida em um dicionário Python nativo usando o método model_dump() e retorna este dicionário como a saída da função triagem.\n",
        "Em resumo, este código cria uma função triagem que utiliza uma cadeia LangChain para enviar uma mensagem do usuário e um prompt do sistema para um modelo de linguagem (llm_triagem). A cadeia é configurada para garantir que a resposta do modelo seja um JSON estruturado de acordo com o modelo TriagemOut, e a função retorna essa saída estruturada como um dicionário."
      ],
      "metadata": {
        "id": "IE-0ADhMcy_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "def triagem(mensagem: str) -> Dict:\n",
        "    saida: TriagemOut = triagem_chain.invoke([\n",
        "        SystemMessage(content=TRIAGEM_PROMPT),\n",
        "        HumanMessage(content=mensagem)\n",
        "    ])\n",
        "\n",
        "    return saida.model_dump()"
      ],
      "metadata": {
        "id": "7anUeX08c4wB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mensagens para o agente"
      ],
      "metadata": {
        "id": "yCQAGxi-fj2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "6xf8q_z5fiYZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fazer um for para ENVIAR CADA MENSAGEm por vez. Este código itera sobre uma lista de mensagens de teste e aplica a função de triagem a cada uma delas.\n",
        "\n",
        "for msg_teste in testes:: Esta linha inicia um loop for que irá percorrer cada item na lista chamada testes. Em cada iteração do loop, o item atual da lista será atribuído à variável msg_teste.\n",
        "print(f\"Pergunta: {msg_teste}\\n -> Resposta: {triagem(msg_teste)}\\n\"): Dentro do loop, esta linha imprime a pergunta atual e a resposta da triagem para essa pergunta.\n",
        "f\"Pergunta: {msg_teste}\\n -> Resposta: {...}\\n\": Usa uma f-string para formatar a saída. Ela imprime o texto \"Pergunta: \" seguido pelo valor da variável msg_teste, uma quebra de linha (\\n), o texto \" -> Resposta: \", o resultado da chamada da função triagem(msg_teste), e outra quebra de linha.\n",
        "triagem(msg_teste): Chama a função triagem (que definimos anteriormente) passando a mensagem de teste atual (msg_teste) como argumento. Esta função processa a mensagem usando o modelo de linguagem e retorna um dicionário com a decisão, urgência e campos faltantes.\n",
        "Em resumo, este código executa a função triagem para cada uma das mensagens na lista testes e imprime o resultado da triagem para cada mensagem de forma clara."
      ],
      "metadata": {
        "id": "OQd2whMnf8iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: {msg_teste}\\n -> Resposta: {triagem(msg_teste)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny8FosLZgI3Y",
        "outputId": "36733b6e-8d90-4126-d671-fd986901a6e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta: Posso reembolsar a internet?\n",
            " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Quero mais 5 dias de trabalho remoto. Como faço?\n",
            " -> Resposta: {'decisao': 'ABRIR_CHAMADO', 'urgencia': 'MEDIA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Posso reembolsar cursos ou treinamentos da Alura?\n",
            " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Quantas capivaras tem no Rio Pinheiros?\n",
            " -> Resposta: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': ['informação sobre a política interna']}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}