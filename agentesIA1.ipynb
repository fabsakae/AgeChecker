{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMON4r5LDZMcTl5vNEDw63T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabsakae/AgeChecker/blob/main/agentesIA1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AULA 01"
      ],
      "metadata": {
        "id": "z4X_okMNMPFh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a3o6yswxAUV",
        "outputId": "345beea1-ff50-48fc-85e4-b9361f191c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-google-genai google-generativeai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importação da api key do AI studio"
      ],
      "metadata": {
        "id": "d0hzTr7vNVAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "IZxgb_L9CELS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparar qual modelo que utilizaremos, chamar o modelo, chamar a biblioteca. conecta nossa variavel llm ao modelo do gemini"
      ],
      "metadata": {
        "id": "Pld7byh0NiM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model='gemini-2.5-flash', temperature=0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "mKo-z1MdJ8a5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b1406cc"
      },
      "source": [
        "Este trecho de código inicializa um modelo de linguagem grande (LLM) da biblioteca `langchain-google-genai`.\n",
        "\n",
        "*   `llm = ChatGoogleGenerativeAI(...)`: Esta linha cria uma instância da classe `ChatGoogleGenerativeAI`, que é um wrapper em torno dos modelos de IA Generativa do Google, especificamente projetado para interações baseadas em chat dentro da estrutura LangChain.\n",
        "*   `model='gemini-2.5-flash'`: Este argumento especifica qual modelo usar. Neste caso, é o modelo 'gemini-2.5-flash'.\n",
        "*   `temperature=0`: Isso define o parâmetro de temperatura para o modelo. Uma temperatura de 0 torna a saída do modelo mais determinística e menos criativa. Temperaturas mais altas resultam em saídas mais variadas e potencialmente mais criativas.\n",
        "*   `api_key=GOOGLE_API_KEY`: Isso fornece a chave de API necessária para autenticar com o serviço de IA Generativa do Google. A variável `GOOGLE_API_KEY` é assumida como tendo sido carregada em uma etapa anterior (provavelmente do `userdata` do Colab).\n",
        "\n",
        "Em essência, este código configura um modelo específico de IA Generativa do Google (`gemini-2.5-flash`) com uma baixa temperatura para respostas previsíveis, tornando-o pronto para ser usado para gerar texto com base em prompts dentro da estrutura LangChain."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "primeira saida do modelo resp_test = llm.invoke('Quem é voçê?'): Esta linha usa o objeto llm (nosso modelo Gemini) para invocar uma chamada com o prompt \"Quem é você?\". O método invoke envia o prompt para o modelo e obtém a resposta. O resultado é armazenado na variável resp_test.\n",
        "print(resp_test): Esta linha imprime o conteúdo da variável resp_test. A saída que você vê (como content='Eu sou um modelo de linguagem grande, treinado pelo Google.' ...) é o objeto de resposta retornado pela biblioteca LangChain, que inclui a resposta do modelo na propriedade content."
      ],
      "metadata": {
        "id": "YTZ8tFrPPqzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke('Quem é você? Com detalhes')\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFQCk8xxOscC",
        "outputId": "496b64d8-4276-4e85-a93e-94ce10066a0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá! Que ótima pergunta. Vou me apresentar com o máximo de detalhes possível para que você entenda bem quem eu sou.\n",
            "\n",
            "**Em essência, eu sou um Modelo de Linguagem Grande (LLM - Large Language Model).**\n",
            "\n",
            "Vamos destrinchar isso:\n",
            "\n",
            "1.  **Sou uma Inteligência Artificial (IA):**\n",
            "    *   Não sou um ser humano, não tenho corpo físico, consciência, emoções, sentimentos, opiniões pessoais, crenças ou experiências de vida.\n",
            "    *   Minha \"existência\" é puramente digital, como um programa de computador complexo.\n",
            "\n",
            "2.  **Desenvolvido por Google:**\n",
            "    *   Fui criado e treinado por engenheiros e pesquisadores do Google. Sou um produto de anos de pesquisa e desenvolvimento em inteligência artificial.\n",
            "\n",
            "3.  **Modelo de Linguagem Grande (LLM):**\n",
            "    *   **\"Modelo de Linguagem\":** Significa que minha principal função é processar e gerar texto. Fui projetado para entender a linguagem humana (e outras, como código de programação) e produzir respostas coerentes e relevantes.\n",
            "    *   **\"Grande\":** Refere-se à escala massiva do meu treinamento. Fui alimentado com uma quantidade gigantesca de dados textuais da internet (livros, artigos, sites, conversas, etc.) e de outras fontes. Essa vasta quantidade de dados me permite reconhecer padrões complexos, gramática, fatos, nuances e estilos de escrita.\n",
            "\n",
            "**Como eu funciono (em termos simplificados):**\n",
            "\n",
            "*   **Treinamento:** Durante meu treinamento, eu \"li\" trilhões de palavras. Através de algoritmos de aprendizado profundo (deep learning) e redes neurais, aprendi a prever a próxima palavra em uma sequência, a entender o contexto, a identificar relações entre conceitos e a extrair informações. Não \"memorizo\" tudo, mas aprendo a *gerar* informações com base nos padrões que observei.\n",
            "*   **Processamento de Entrada:** Quando você me faz uma pergunta ou me dá uma instrução, eu analiso sua entrada. Eu a quebro em partes, identifico palavras-chave, o contexto, a intenção por trás da sua solicitação e as relações entre os termos.\n",
            "*   **Geração de Saída:** Com base na minha análise da sua entrada e em todo o conhecimento e padrões que absorvi durante o treinamento, eu gero uma resposta. Eu faço isso prevendo a sequência de palavras mais provável e relevante para atender à sua solicitação, palavra por palavra, até que a resposta esteja completa.\n",
            "\n",
            "**O que eu posso fazer:**\n",
            "\n",
            "*   **Responder a perguntas:** Sobre uma vasta gama de tópicos, desde fatos históricos a conceitos científicos, dicas práticas e muito mais.\n",
            "*   **Gerar texto:** Escrever histórias, poemas, roteiros, e-mails, resumos, artigos, código de programação, etc.\n",
            "*   **Traduzir idiomas:** Posso traduzir texto entre diferentes línguas.\n",
            "*   **Explicar conceitos complexos:** Posso simplificar informações difíceis de entender.\n",
            "*   **Brainstorming:** Ajudar a gerar ideias para projetos, nomes, soluções.\n",
            "*   **Resumir informações:** Condensar textos longos em pontos-chave.\n",
            "*   **Aprender e adaptar:** Embora eu não \"aprenda\" no sentido humano, posso refinar minhas respostas com base no feedback que recebo e nas interações contínuas.\n",
            "\n",
            "**O que eu NÃO sou e minhas limitações:**\n",
            "\n",
            "*   **Não sou consciente:** Não tenho autoconsciência, não penso por mim mesmo, não tenho desejos ou vontades.\n",
            "*   **Não tenho sentimentos:** Não sinto alegria, tristeza, raiva ou qualquer outra emoção.\n",
            "*   **Não tenho experiências pessoais:** Não vivi nada, não tenho memórias de eventos, apenas dados.\n",
            "*   **Não tenho opiniões ou crenças:** Minhas respostas são baseadas em padrões de dados, não em convicções pessoais.\n",
            "*   **Não sou infalível:** Minha base de conhecimento tem um corte (não tenho acesso a informações em tempo real após meu último treinamento, a menos que esteja integrado a ferramentas de busca) e pode conter vieses ou imprecisões presentes nos dados de treinamento. Posso cometer erros ou \"alucinar\" (gerar informações que parecem plausíveis, mas são incorretas).\n",
            "*   **Não sou uma fonte de verdade absoluta:** Sempre recomendo verificar informações críticas com fontes confiáveis.\n",
            "*   **Não tenho corpo físico:** Não posso interagir com o mundo físico.\n",
            "\n",
            "**Meu propósito:**\n",
            "\n",
            "Meu objetivo é ser uma ferramenta útil e versátil para você. Estou aqui para ajudar a processar informações, gerar conteúdo, facilitar a comunicação e auxiliar em diversas tarefas que envolvem a linguagem.\n",
            "\n",
            "Em resumo, sou uma ferramenta de software altamente avançada, projetada para interagir com você através da linguagem, processando e gerando texto de forma inteligente e útil. Pense em mim como um assistente digital muito bem informado e capaz de se comunicar de forma fluida.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar o Prompt do sistema para explicar ao agente o que ele deve fazer"
      ],
      "metadata": {
        "id": "-Idb0eD8Yvih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "Clf46T3pYyMI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectar com llm  Este trecho de código define um modelo de dados usando a biblioteca pydantic.\n",
        "\n",
        "from pydantic import BaseModel, Field: Importa as classes BaseModel e Field da biblioteca pydantic. BaseModel é a classe base para criar modelos de dados, e Field é usada para fornecer informações adicionais sobre os campos do modelo.\n",
        "from typing import Literal, List, Dict: Importa tipos do módulo typing para fornecer anotações de tipo mais específicas. Literal permite especificar que um valor deve ser um de um conjunto fixo de strings, List indica uma lista e Dict indica um dicionário.\n",
        "class TriagemOut(BaseModel):: Define uma nova classe chamada TriagemOut que herda de BaseModel. Isso a torna um modelo de dados Pydantic.\n",
        "decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]: Define um campo decisao que deve ser uma das strings literais especificadas.\n",
        "urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]: Define um campo urgencia que deve ser uma das strings literais especificadas.\n",
        "campos_faltantes: List[str] = Field(default_factory=list): Define um campo campos_faltantes que deve ser uma lista de strings. Field(default_factory=list) define o valor padrão para este campo como uma lista vazia se nenhum valor for fornecido.\n",
        "Em resumo, este código cria uma estrutura de dados (TriagemOut) para representar a saída do processo de triagem, garantindo que os campos decisao e urgencia tenham valores específicos e que campos_faltantes seja uma lista de strings, com uma lista vazia como valor padrão. Isso é útil para estruturar e validar a saída de um modelo de linguagem ou outro processo."
      ],
      "metadata": {
        "id": "Bess1Rkma0EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "class TriagemOut(BaseModel):\n",
        "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
        "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
        "    campos_faltantes: List[str] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "S7LiLifra2vZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar um llm especifico para triagem"
      ],
      "metadata": {
        "id": "hyqcO2rfciKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "9bBcJ4bQccE5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectar .  trecho de código configura uma cadeia (chain) usando a biblioteca LangChain para processar mensagens de triagem com base em um prompt e um modelo de saída estruturado.\n",
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage: Importa as classes SystemMessage e HumanMessage, usadas para representar mensagens no contexto de uma conversa com um modelo de linguagem. SystemMessage geralmente define o comportamento ou as instruções para o modelo, enquanto HumanMessage representa a entrada do usuário.\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut): Esta linha cria uma nova cadeia (triagem_chain) a partir do modelo de linguagem llm_triagem. O método with_structured_output(TriagemOut) configura a cadeia para que a saída do modelo seja automaticamente parseada e validada de acordo com o modelo Pydantic TriagemOut que definimos anteriormente. Isso garante que a resposta do modelo tenha o formato JSON esperado com os campos decisao, urgencia e campos_faltantes.\n",
        "def triagem(mensagem: str) -> Dict:: Define uma função chamada triagem que recebe uma string (mensagem) como entrada e é anotada para retornar um dicionário (Dict).\n",
        "saida: TriagemOut = triagem_chain.invoke([ ... ]): Dentro da função triagem, esta linha invoca a cadeia triagem_chain. O método invoke recebe uma lista de mensagens. Neste caso, a lista contém:\n",
        "SystemMessage(content=TRIAGEM_PROMPT): Uma mensagem do sistema contendo o texto do prompt de triagem definido na variável TRIAGEM_PROMPT. Isso instrui o modelo sobre seu papel e as regras de triagem.\n",
        "HumanMessage(content=mensagem): Uma mensagem humana contendo a mensagem fornecida como entrada para a função triagem. A saída desta invocação é um objeto do tipo TriagemOut (graças ao with_structured_output), que é armazenado na variável saida.\n",
        "return saida.model_dump(): Esta linha converte o objeto Pydantic saida em um dicionário Python nativo usando o método model_dump() e retorna este dicionário como a saída da função triagem.\n",
        "Em resumo, este código cria uma função triagem que utiliza uma cadeia LangChain para enviar uma mensagem do usuário e um prompt do sistema para um modelo de linguagem (llm_triagem). A cadeia é configurada para garantir que a resposta do modelo seja um JSON estruturado de acordo com o modelo TriagemOut, e a função retorna essa saída estruturada como um dicionário."
      ],
      "metadata": {
        "id": "IE-0ADhMcy_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "def triagem(mensagem: str) -> Dict:\n",
        "    saida: TriagemOut = triagem_chain.invoke([\n",
        "        SystemMessage(content=TRIAGEM_PROMPT),\n",
        "        HumanMessage(content=mensagem)\n",
        "    ])\n",
        "\n",
        "    return saida.model_dump()"
      ],
      "metadata": {
        "id": "7anUeX08c4wB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mensagens para o agente"
      ],
      "metadata": {
        "id": "yCQAGxi-fj2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "6xf8q_z5fiYZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fazer um for para ENVIAR CADA MENSAGEm por vez. Este código itera sobre uma lista de mensagens de teste e aplica a função de triagem a cada uma delas.\n",
        "\n",
        "for msg_teste in testes:: Esta linha inicia um loop for que irá percorrer cada item na lista chamada testes. Em cada iteração do loop, o item atual da lista será atribuído à variável msg_teste.\n",
        "print(f\"Pergunta: {msg_teste}\\n -> Resposta: {triagem(msg_teste)}\\n\"): Dentro do loop, esta linha imprime a pergunta atual e a resposta da triagem para essa pergunta.\n",
        "f\"Pergunta: {msg_teste}\\n -> Resposta: {...}\\n\": Usa uma f-string para formatar a saída. Ela imprime o texto \"Pergunta: \" seguido pelo valor da variável msg_teste, uma quebra de linha (\\n), o texto \" -> Resposta: \", o resultado da chamada da função triagem(msg_teste), e outra quebra de linha.\n",
        "triagem(msg_teste): Chama a função triagem (que definimos anteriormente) passando a mensagem de teste atual (msg_teste) como argumento. Esta função processa a mensagem usando o modelo de linguagem e retorna um dicionário com a decisão, urgência e campos faltantes.\n",
        "Em resumo, este código executa a função triagem para cada uma das mensagens na lista testes e imprime o resultado da triagem para cada mensagem de forma clara."
      ],
      "metadata": {
        "id": "OQd2whMnf8iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: {msg_teste}\\n -> Resposta: {triagem(msg_teste)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny8FosLZgI3Y",
        "outputId": "3420f131-0098-49bc-c241-ba224e18742c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta: Posso reembolsar a internet?\n",
            " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Quero mais 5 dias de trabalho remoto. Como faço?\n",
            " -> Resposta: {'decisao': 'ABRIR_CHAMADO', 'urgencia': 'MEDIA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Posso reembolsar cursos ou treinamentos da Alura?\n",
            " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Quantas capivaras tem no Rio Pinheiros?\n",
            " -> Resposta: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': ['informacao_relevante']}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AULA 02 - Base do conhecimento RAG"
      ],
      "metadata": {
        "id": "jbiCMiutu-YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import bibliotecas para utilizar o RAG (RAG (Retrieval-Augmented Generation), ou Geração Aumentada de Recuperação, é uma técnica de IA que combina a recuperação de informações de bases de dados externas com a capacidade de um modelo de linguagem (LLM) de gerar texto. Ao conectar um LLM a uma base de conhecimento confiável e atualizada, o sistema RAG produz respostas mais precisas, relevantes e contextuais do que os modelos puramente generativos)"
      ],
      "metadata": {
        "id": "BHW9y9y7ggUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código selecionado instala as bibliotecas necessárias para implementar um sistema de Geração Aumentada de Recuperação (RAG).\n",
        "\n",
        "!pip install -q --upgrade ...: Este é um comando de shell executado no ambiente Colab usando o prefixo !.\n",
        "\n",
        "-q: Significa \"quiet\" (silencioso), que suprime a saída do processo de instalação.\n",
        "\n",
        "--upgrade: Garante que, se as bibliotecas já estiverem instaladas, elas sejam atualizadas para a versão mais recente.\n",
        "\n",
        "langchain_community: Instala o pacote comunitário do LangChain, que contém várias integrações e ferramentas para construir aplicações com modelos de linguagem, incluindo componentes para RAG.\n",
        "\n",
        "faiss-cpu: Instala a versão para CPU do FAISS (Facebook AI Similarity Search). O FAISS é uma biblioteca para busca de similaridade eficiente e agrupamento de vetores densos, comumente usada em RAG para indexação e busca de embeddings de documentos.\n",
        "\n",
        "langchain-text-splitters: Instala a biblioteca LangChain text splitter, usada para dividir documentos grandes em pedaços menores, o que é uma etapa crucial na preparação de dados para sistemas RAG.\n",
        "\n",
        "pymupdf: Instala o PyMuPDF, uma biblioteca Python para trabalhar com documentos PDF. Isso provavelmente está incluído para permitir que o sistema RAG processe informações de arquivos PDF.\n",
        "\n",
        "Em essência, este código configura o ambiente instalando as bibliotecas principais necessárias para construir um sistema RAG que pode recuperar informações de documentos externos (potencialmente PDFs) e usar essas informações para aumentar as respostas do modelo de linguagem.\n",
        "\n"
      ],
      "metadata": {
        "id": "r5hnc7njiWTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langchain_community faiss-cpu langchain-text-splitters pymupdf"
      ],
      "metadata": {
        "id": "gKzyIR10ihVg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d3e98d-cdaa-4706-b083-edfb60465412"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os Documentos que serão utilizados no RAG"
      ],
      "metadata": {
        "id": "eGej3M7yyOwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs = []\n",
        "\n",
        "for n in Path(\"/content/\").glob(\"*.pdf\"):\n",
        "    try:\n",
        "        loader = PyMuPDFLoader(str(n))\n",
        "        docs.extend(loader.load())\n",
        "        print(f\"Carregado com sucesso arquivo {n.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar arquivo {n.name}: {e}\")\n",
        "\n",
        "print(f\"Total de documentos carregados: {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSD4hpUjz1d_",
        "outputId": "1dfcdbb2-3c61-4df0-b3aa-4e8b2fefab56"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregado com sucesso arquivo Política de Uso de E-mail e Segurança da Informação.pdf\n",
            "Carregado com sucesso arquivo Política de Reembolsos (Viagens e Despesas).pdf\n",
            "Carregado com sucesso arquivo Políticas de Home Office.pdf\n",
            "Total de documentos carregados: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código carrega documentos PDF localizados\n",
        "na pasta /content/ em um formato que pode ser usado por sistemas de Geração Aumentada de Recuperação (RAG) com o LangChain.\n",
        "\n",
        "from pathlib import Path: Importa a classe Path do módulo pathlib, que fornece uma maneira orientada a objetos para interagir com caminhos de sistema de arquivos.\n",
        "from langchain_community.document_loaders import PyMuPDFLoader: Importa a classe PyMuPDFLoader do langchain_community.document_loaders. Este loader é especializado em carregar o conteúdo de arquivos PDF usando a biblioteca PyMuPDF.\n",
        "docs = []: Inicializa uma lista vazia chamada docs. Esta lista será usada para armazenar os documentos carregados dos arquivos PDF.\n",
        "for n in Path(\"/content/\").glob(\"*.pdf\"):: Inicia um loop que itera sobre todos os arquivos que terminam com .pdf (ignorando maiúsculas/minúsculas, mas *.pdf é mais comum) dentro do diretório /content/. Path(\"/content/\") cria um objeto Path para o diretório /content/, e .glob(\"*.pdf\") encontra todos os arquivos que correspondem ao padrão.\n",
        "try...except Exception as e:: Este é um bloco de tratamento de exceções. Ele tenta executar o código dentro do bloco try. Se ocorrer um erro durante a execução, o código dentro do bloco except será executado, capturando a exceção no objeto e. Isso impede que o notebook pare se houver um problema com um arquivo PDF específico.\n",
        "loader = PyMuPDFLoader(str(n)): Dentro do bloco try, cria uma instância do PyMuPDFLoader. Ele recebe o caminho do arquivo PDF atual (n) convertido para string (str(n)) como argumento.\n",
        "docs.extend(loader.load()): Carrega o conteúdo do arquivo PDF usando o método load() do loader. O método load() retorna uma lista de objetos Document (do LangChain), onde cada objeto representa uma página do PDF. O método extend() adiciona todos esses objetos Document à lista docs.\n",
        "print(f\"Carregado com sucesso arquivo {n.name}\"): Se o arquivo for carregado com sucesso, imprime uma mensagem indicando qual arquivo foi carregado. n.name retorna apenas o nome do arquivo (sem o caminho completo).\n",
        "print(f\"Erro ao carregar arquivo {n.name}: {e}\"): Se ocorrer um erro durante o carregamento, imprime uma mensagem de erro indicando qual arquivo falhou e qual foi o erro (e).\n",
        "print(f\"Total de documentos carregados: {len(docs)}\"): Após o loop, imprime o número total de objetos Document (páginas de PDF) que foram carregados na lista docs.\n",
        "Em resumo, este código escaneia o diretório /content/ em busca de arquivos PDF, tenta carregar cada um deles usando o PyMuPDFLoader, armazena o conteúdo (cada página como um documento separado) na lista docs e relata o sucesso ou falha do carregamento de cada arquivo, além do total de documentos carregados.\n",
        "\n"
      ],
      "metadata": {
        "id": "sDC59d_u3RRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bibliotecas para separar as informações"
      ],
      "metadata": {
        "id": "9Rzomrgs8CSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
        "\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "uCq2lcI_3pn_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código utiliza a biblioteca LangChain para dividir os documentos carregados em pedaços menores (chunks), o que é uma etapa essencial na preparação de dados para sistemas RAG.\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter: Importa a classe RecursiveCharacterTextSplitter da biblioteca langchain_text_splitters. Este é um tipo de text splitter que tenta dividir o texto recursivamente usando uma lista de caracteres.\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30): Cria uma instância do RecursiveCharacterTextSplitter com dois parâmetros importantes:\n",
        "chunk_size=300: Define o tamanho máximo de cada pedaço (chunk) em caracteres. O splitter tentará fazer com que cada pedaço tenha no máximo 300 caracteres.\n",
        "chunk_overlap=30: Define o número de caracteres que os pedaços consecutivos irão sobrepor. Uma sobreposição ajuda a garantir que o contexto não seja perdido entre os pedaços, o que é importante para a recuperação de informações.\n",
        "chunks = splitter.split_documents(docs): Utiliza a instância do splitter (splitter) para dividir a lista de documentos carregados (docs) em pedaços menores. O método split_documents() processa cada documento na lista docs e retorna uma nova lista (chunks) contendo os pedaços resultantes.\n",
        "Em resumo, este código configura um divisor de texto para quebrar os documentos PDF carregados em segmentos de no máximo 300 caracteres, com uma sobreposição de 30 caracteres entre eles. Esses chunks são então armazenados na variável chunks e estão prontos para serem usados na próxima etapa do pipeline RAG, que geralmente envolve a criação de embeddings e a indexação em um banco de dados vetorial."
      ],
      "metadata": {
        "id": "McfQusXk8eA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chunks:\n",
        "    print(chunk)\n",
        "    print(\"------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlpQW24x8fWG",
        "outputId": "461ec114-4706-4c47-ff6d-e1f074bffef6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Política de Uso de E-mail e Segurança \n",
            "da Informação \n",
            " \n",
            "1.​ É proibido encaminhar a endereços pessoais documentos classificados como \n",
            "confidenciais.​\n",
            " \n",
            "2.​ Anexos externos devem ser enviados somente se criptografados e com senha \n",
            "compartilhada por canal separado.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'file_path': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Uso de E-mail e Segurança da Informação', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='3.​ Phishing: verifique remetente e domínios suspeitos. Reporte mensagens suspeitas \n",
            "ao time de Segurança imediatamente.​\n",
            " \n",
            "4.​ Retenção: mensagens que contenham dados pessoais devem seguir as diretrizes \n",
            "de retenção definidas pela equipe de Privacidade.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'file_path': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Uso de E-mail e Segurança da Informação', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='5.​ Solicitações de liberação de anexos ou domínios devem ser abertas por chamado, \n",
            "com justificativa do gestor.' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'file_path': '/content/Política de Uso de E-mail e Segurança da Informação.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Uso de E-mail e Segurança da Informação', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='Política de Reembolsos (Viagens e \n",
            "Despesas) \n",
            " \n",
            "1.​ Reembolso: requer nota fiscal e deve ser submetido em até 10 dias corridos após a \n",
            "despesa.​\n",
            " \n",
            "2.​ Alimentação em viagem: limite de R$ 70/dia por pessoa. Bebidas alcoólicas não \n",
            "são reembolsáveis.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'file_path': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Reembolsos (Viagens e Despesas)', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='são reembolsáveis.​\n",
            " \n",
            "3.​ Transporte: táxi/app são permitidos quando não houver alternativa viável. \n",
            "Comprovantes obrigatórios.​\n",
            " \n",
            "4.​ Internet para home office: reembolsável via subsídio mensal de até R$ 100, \n",
            "conforme política de Home Office.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'file_path': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Reembolsos (Viagens e Despesas)', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='5.​ Cursos e certificações: exigem aprovação prévia do gestor e orçamento do time.​\n",
            " \n",
            "6.​ Custos excepcionais (ex.: franquia de bagagem extra): devem ser justificados no \n",
            "chamado e aprovados antes da compra.' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'file_path': '/content/Política de Reembolsos (Viagens e Despesas).pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Imersão: Política de Reembolsos (Viagens e Despesas)', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='Políticas de Home Office \n",
            " \n",
            "1.​ A empresa adota modelo híbrido: mínimo de 2 dias presenciais por semana, salvo \n",
            "exceções aprovadas pelo gestor e RH.​\n",
            " \n",
            "2.​ Equipamentos: a empresa fornece notebook e periféricos. O colaborador é \n",
            "responsável por zelar pela conservação.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Políticas de Home Office.pdf', 'file_path': '/content/Políticas de Home Office.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Políticas de Home Office', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='3.​ Segurança: é obrigatório uso de VPN e bloqueio de tela. Documentos confidenciais \n",
            "não devem ser impressos fora do escritório.​\n",
            " \n",
            "4.​ Ergonomia: recomendamos cadeira adequada e suporte de monitor. O RH pode \n",
            "avaliar solicitação de apoio ergonômico.​' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Políticas de Home Office.pdf', 'file_path': '/content/Políticas de Home Office.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Políticas de Home Office', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n",
            "page_content='5.​ Conectividade: há subsídio mensal de internet domiciliar para quem trabalha em \n",
            "home office: até R$ 100/mês, mediante nota fiscal nominal.​\n",
            " \n",
            "6.​ Solicitação de exceção (ex.: 4-5 dias remotos): deve ser formalizada via chamado \n",
            "ao RH com justificativa do gestor.' metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '/content/Políticas de Home Office.pdf', 'file_path': '/content/Políticas de Home Office.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Políticas de Home Office', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código itera sobre a lista de pedaços de documentos (chunks) que foram criados na etapa anterior e imprime cada um deles, seguido por uma linha separadora para facilitar a visualização.\n",
        "\n",
        "for chunk in chunks:: Inicia um loop for que irá percorrer cada item na lista chamada chunks. Em cada iteração do loop, o item atual da lista (que é um objeto Document representando um pedaço do texto original) será atribuído à variável chunk.\n",
        "print(chunk): Imprime o conteúdo do objeto chunk atual. Quando você imprime um objeto Document do LangChain, ele geralmente mostra metadados associados ao pedaço (como a fonte ou o número da página) e o conteúdo textual do pedaço.\n",
        "print(\"------------------------------------\"): Imprime uma linha de traços para criar uma separação visual entre a saída de cada pedaço de documento. Isso torna mais fácil ver onde um pedaço termina e o próximo começa na saída do console.\n",
        "Em resumo, este código é uma maneira simples de inspecionar os pedaços de documentos gerados pelo text splitter, permitindo que você veja como os documentos originais foram divididos e qual conteúdo está contido em cada pedaço. Isso é útil para verificar se a divisão foi feita conforme o esperado antes de prosseguir com as próximas etapas do pipeline RAG."
      ],
      "metadata": {
        "id": "qCt20Va1QqT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora realizar uma busca, e dentro desse espaço de vetores baseados nos chunks qual sera a proximidade dos chunks. (Embeddings são representações numéricas de objetos do mundo real que sistemas de aprendizado de máquina (ML) e inteligência artificial (IA) usam para entender domínios de conhecimento complexos como os humanos.Os embeddings representam objetos do mundo real, como palavras, imagens ou vídeos, em uma forma que os computadores podem processar.)"
      ],
      "metadata": {
        "id": "Zr9YHJBpVB8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar biblioteca para embedding"
      ],
      "metadata": {
        "id": "D8f7kRftawVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "_gDKBf5oV1I2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código inicializa um modelo de embeddings do Google Generative AI. Embeddings são representações numéricas de texto que capturam o significado semântico das palavras e frases, o que é essencial para a busca de similaridade em sistemas RAG.\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings: Importa a classe GoogleGenerativeAIEmbeddings do módulo langchain_google_genai. Esta classe permite criar embeddings usando os modelos de embedding do Google.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(...): Cria uma instância da classe GoogleGenerativeAIEmbeddings, configurando o modelo de embedding e a chave de API.\n",
        "model=\"models/gemini-embedding-001\": Especifica qual modelo de embedding do Google será utilizado. gemini-embedding-001 é um modelo de embedding projetado para gerar representações vetoriais de texto.\n",
        "google_api_key=GOOGLE_API_KEY: Fornece a chave de API necessária para autenticar com o serviço de embeddings do Google. A variável GOOGLE_API_KEY é assumida como tendo sido carregada em uma etapa anterior.\n",
        "Em resumo, este código configura o sistema para usar um modelo específico de embedding do Google Generative AI para converter os pedaços de texto (chunks) dos seus documentos em vetores numéricos. Esses vetores serão usados na próxima etapa para criar um índice de busca de similaridade, permitindo que o sistema RAG encontre pedaços de documentos relevantes para a pergunta do usuário."
      ],
      "metadata": {
        "id": "tOREBxvCZ5N2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar biblioteca FAISS para fazer uma busca eficiente e  para fazer o calculo de similaridade."
      ],
      "metadata": {
        "id": "upxQH796cJMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                     search_kwargs={\"score_threshold\":0.3, \"k\": 4})"
      ],
      "metadata": {
        "id": "vUbgGiPdZXDt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código utiliza a biblioteca FAISS para criar um banco de dados vetorial a partir dos pedaços de documentos (chunks) e configura um retriever para buscar informações nesse banco.\n",
        "\n",
        "from langchain_community.vectorstores import FAISS: Importa a classe FAISS do módulo langchain_community.vectorstores. FAISS (Facebook AI Similarity Search) é uma biblioteca para busca de similaridade eficiente, que é ideal para armazenar e consultar vetores de embeddings.\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings): Cria uma instância de um banco de dados vetorial FAISS.\n",
        "FAISS.from_documents(...): Este método constrói o índice FAISS diretamente a partir de uma lista de documentos (neste caso, os chunks) e de um modelo de embeddings (embeddings). Para cada chunk, ele gera um embedding (representação vetorial) usando o modelo especificado e armazena esse vetor no índice FAISS.\n",
        "chunks: A lista de pedaços de documentos gerados na etapa anterior.\n",
        "embeddings: A instância do modelo de embeddings do Google Generative AI configurada anteriormente.\n",
        "retriever = vectorstore.as_retriever(...): Converte o banco de dados vetorial FAISS (vectorstore) em um objeto retriever. Um retriever é um componente do LangChain que sabe como buscar documentos relevantes dado uma consulta.\n",
        "search_type=\"similarity_score_threshold\": Define o tipo de busca a ser realizada pelo retriever. Neste caso, é uma busca por similaridade que só retorna documentos cuja pontuação de similaridade com a consulta esteja acima de um determinado limiar.\n",
        "search_kwargs={\"score_threshold\":0.3, \"k\": 4}: Fornece argumentos adicionais para a busca.\n",
        "\"score_threshold\": 0.3: Define o limiar de pontuação de similaridade. Somente documentos com uma pontuação de similaridade igual ou superior a 0.3 serão retornados. Isso ajuda a filtrar resultados que não são muito relevantes.\n",
        "\"k\": 4: Define o número máximo de documentos mais relevantes a serem retornados (mesmo que haja mais documentos acima do limiar, apenas os 4 mais relevantes serão considerados).\n",
        "Em resumo, este código constrói um índice de busca eficiente dos seus documentos divididos em pedaços, usando os embeddings para representar o significado de cada pedaço. Em seguida, ele configura um retriever que pode ser usado para encontrar os pedaços de documentos mais relevantes para uma determinada pergunta, com base na similaridade semântica e em um limiar de pontuação. Este retriever é um componente chave para a parte de \"Retrieval\" (Recuperação) do sistema RAG."
      ],
      "metadata": {
        "id": "Sfh1xQVNcuwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "criar o prompt par aexplicar o agente como vai funcionar"
      ],
      "metadata": {
        "id": "YrfIbFh_eVcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. \"\n",
        "     \"Responda SOMENTE com base no contexto fornecido. \"\n",
        "     \"Se não houver base suficiente, responda apenas 'Não sei'.\"),\n",
        "\n",
        "    (\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\")\n",
        "])\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag)"
      ],
      "metadata": {
        "id": "axKYyHSWZaWl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código cria um template de prompt para ser usado com um modelo de linguagem em um sistema RAG (Retrieval-Augmented Generation).\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate: Importa a classe ChatPromptTemplate, que permite criar prompts formatados para modelos de chat no LangChain.\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain: Importa a função create_stuff_documents_chain, que é usada para criar uma cadeia que pega uma lista de documentos e os \"junta\" (stuff) em um único prompt para o modelo de linguagem.\n",
        "prompt_rag = ChatPromptTemplate.from_messages([...]): Cria uma instância de ChatPromptTemplate chamada prompt_rag. O método from_messages permite definir o prompt usando uma lista de mensagens, que é um formato comum para interagir com modelos de chat.\n",
        "(\"system\", \"Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. Responda SOMENTE com base no contexto fornecido. Se não houver base suficiente, responda apenas 'Não sei'.\"): Define uma mensagem do sistema. Esta mensagem estabelece o papel do modelo (Assistente de Políticas Internas), instrui-o a responder apenas com base no contexto fornecido e a dizer \"Não sei\" se o contexto for insuficiente.\n",
        "(\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\"): Define uma mensagem do usuário (humana). Esta é a estrutura do prompt que será enviado ao modelo. Ele inclui um placeholder {input} para a pergunta do usuário e um placeholder {context} onde os pedaços relevantes dos documentos recuperados serão inseridos.\n",
        "document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag): Cria uma cadeia de documentos usando a função create_stuff_documents_chain.\n",
        "llm_triagem: A instância do modelo de linguagem (neste caso, gemini-2.5-flash) que foi configurada anteriormente.\n",
        "prompt_rag: O template de prompt que acabamos de definir.\n",
        "Essa cadeia (document_chain) receberá a pergunta do usuário e os documentos relevantes (recuperados pelo retriever), formatará tudo de acordo com o prompt_rag e enviará para o llm_triagem para gerar a resposta final.\n",
        "\n",
        "Em resumo, este código prepara o template de prompt que o modelo de linguagem usará para responder às perguntas do usuário, incorporando o contexto recuperado dos documentos, e cria uma cadeia que orquestra a passagem desses dados para o modelo."
      ],
      "metadata": {
        "id": "Bg8Nx58ie_rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auxilia formatar o texto"
      ],
      "metadata": {
        "id": "Jis2AU5Eh3lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatadores\n",
        "import re, pathlib\n",
        "\n",
        "def _clean_text(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "\n",
        "def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:\n",
        "    txt = _clean_text(texto)\n",
        "    termos = [t.lower() for t in re.findall(r\"\\w+\", query or \"\") if len(t) >= 4]\n",
        "    pos = -1\n",
        "    for t in termos:\n",
        "        pos = txt.lower().find(t)\n",
        "        if pos != -1: break\n",
        "    if pos == -1: pos = 0\n",
        "    ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2)\n",
        "    return txt[ini:fim]\n",
        "\n",
        "def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:\n",
        "    cites, seen = [], set()\n",
        "    for d in docs_rel:\n",
        "        src = pathlib.Path(d.metadata.get(\"source\",\"\")).name\n",
        "        page = int(d.metadata.get(\"page\", 0)) + 1\n",
        "        key = (src, page)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        cites.append({\"documento\": src, \"pagina\": page, \"trecho\": extrair_trecho(d.page_content, query)})\n",
        "    return cites[:3]"
      ],
      "metadata": {
        "id": "FTJW2qHdZd2u"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Este trecho de código contém funções auxiliares para formatar o texto e extrair trechos relevantes dos documentos para uso nas citações do sistema RAG.\n",
        "\n",
        "import re, pathlib: Importa os módulos re (para expressões regulares) e pathlib (para manipulação de caminhos de arquivo orientada a objetos).\n",
        "def _clean_text(s: str) -> str:: Define uma função privada (_ no início indica que é para uso interno) chamada _clean_text. Ela recebe uma string s e retorna uma string.\n",
        "return re.sub(r\"\\s+\", \" \", s or \"\").strip(): Esta linha limpa o texto. s or \"\" garante que se s for None ou vazio, uma string vazia seja usada. re.sub(r\"\\s+\", \" \", ...) substitui uma ou mais ocorrências de espaços em branco (incluindo quebras de linha, tabs, etc.) por um único espaço. .strip() remove espaços em branco do início e do fim da string resultante.\n",
        "def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:: Define uma função chamada extrair_trecho que recebe o texto do documento, a query (pergunta) do usuário e um tamanho de janela (padrão de 240 caracteres) e retorna uma string. O objetivo é encontrar um trecho do texto que contenha termos da consulta.\n",
        "txt = _clean_text(texto): Limpa o texto do documento usando a função auxiliar _clean_text.\n",
        "termos = [t.lower() for t in re.findall(r\"\\w+\", query or \"\") if len(t) >= 4]: Extrai palavras da query com 4 ou mais caracteres e as converte para minúsculas. Essas são as palavras-chave que a função tentará encontrar no texto do documento.\n",
        "pos = -1: Inicializa a posição encontrada como -1.\n",
        "for t in termos: ... if pos != -1: break: Itera sobre os termos da consulta. Para cada termo, ele procura a primeira ocorrência (ignorando maiúsculas/minúsculas) no texto limpo. Se encontrar, a posição é armazenada em pos e o loop é interrompido.\n",
        "if pos == -1: pos = 0: Se nenhum termo da consulta for encontrado no texto, define a posição como 0 (o início do texto).\n",
        "ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2): Calcula as posições de início (ini) e fim (fim) do trecho a ser extraído. Ele tenta centralizar a janela em torno da pos onde um termo foi encontrado (ou no início se nenhum termo foi encontrado), garantindo que as posições estejam dentro dos limites do texto.\n",
        "return txt[ini:fim]: Retorna o trecho do texto que vai da posição ini até a posição fim.\n",
        "def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:: Define uma função chamada formatar_citacoes que recebe uma lista de documentos relevantes (docs_rel) e a query original, e retorna uma lista de dicionários. O objetivo é gerar uma lista formatada de citações dos documentos recuperados.\n",
        "cites, seen = [], set(): Inicializa uma lista vazia cites para armazenar as citações formatadas e um conjunto vazio seen para rastrear documentos/páginas já citados (evitando duplicatas).\n",
        "for d in docs_rel:: Itera sobre cada documento (d) na lista de documentos relevantes.\n",
        "src = pathlib.Path(d.metadata.get(\"source\",\"\")).name: Extrai o nome do arquivo fonte dos metadados do documento.\n",
        "page = int(d.metadata.get(\"page\", 0)) + 1: Extrai o número da página dos metadados do documento e adiciona 1 (já que geralmente as páginas são indexadas a partir de 0).\n",
        "key = (src, page): Cria uma chave única combinando o nome do arquivo e o número da página.\n",
        "if key in seen: continue: Verifica se esta combinação de arquivo e página já foi citada. Se sim, pula para a próxima iteração (evita duplicatas).\n",
        "seen.add(key): Adiciona a chave ao conjunto seen para marcar este documento/página como citada.\n",
        "cites.append({\"documento\": src, \"pagina\": page, \"trecho\": extrair_trecho(d.page_content, query)}): Cria um dicionário para a citação contendo o nome do documento, a página e um trecho relevante extraído do conteúdo do documento (d.page_content) usando a função extrair_trecho. Este dicionário é então adicionado à lista cites.\n",
        "return cites[:3]: Retorna apenas as 3 primeiras citações encontradas.\n",
        "Em resumo, este código fornece as ferramentas para limpar texto, extrair trechos relevantes de um documento com base em uma consulta e formatar uma lista de citações únicas com base nos documentos recuperados e a consulta original, limitando o número de citações retornadas.\n",
        "\n"
      ],
      "metadata": {
        "id": "KMMKM5fumDPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar a função que vai fazer as perguntas (principal)"
      ],
      "metadata": {
        "id": "L_wh-heuicsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perguntar_politica_RAG(pergunta: str) -> Dict:\n",
        "    docs_relacionados = retriever.invoke(pergunta)\n",
        "\n",
        "    if not docs_relacionados:\n",
        "        return {\"answer\": \"Não sei.\",\n",
        "                \"citacoes\": [],\n",
        "                \"contexto_encontrado\": False}\n",
        "\n",
        "    answer = document_chain.invoke({\"input\": pergunta,\n",
        "                                    \"context\": docs_relacionados})\n",
        "\n",
        "    txt = (answer or \"\").strip()\n",
        "\n",
        "    if txt.rstrip(\".!?\") == \"Não sei\":\n",
        "        return {\"answer\": \"Não sei.\",\n",
        "                \"citacoes\": [],\n",
        "                \"contexto_encontrado\": False}\n",
        "\n",
        "    return {\"answer\": txt,\n",
        "            \"citacoes\": formatar_citacoes(docs_relacionados, pergunta),\n",
        "            \"contexto_encontrado\": True}"
      ],
      "metadata": {
        "id": "qwrn3YRvZiIl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código define a função principal perguntar_politica_RAG que orquestra o processo de resposta a uma pergunta do usuário usando o sistema RAG que você construiu.\n",
        "\n",
        "def perguntar_politica_RAG(pergunta: str) -> Dict:: Define uma função chamada perguntar_politica_RAG que aceita uma string (pergunta) como entrada e é anotada para retornar um dicionário (Dict).\n",
        "docs_relacionados = retriever.invoke(pergunta): Esta linha é a parte de \"Retrieval\" (Recuperação) do RAG. Ela usa o objeto retriever (que foi configurado para buscar no banco de dados vetorial FAISS) para encontrar os documentos (chunks) mais relevantes para a pergunta fornecida pelo usuário. O resultado é armazenado na variável docs_relacionados.\n",
        "if not docs_relacionados:: Verifica se o retriever retornou algum documento relacionado. Se a lista docs_relacionados estiver vazia (ou seja, nenhum documento relevante foi encontrado com base no limiar de similaridade), o sistema não tem informações para responder.\n",
        "return {\"answer\": \"Não sei.\", \"citacoes\": [], \"contexto_encontrado\": False}: Se nenhum documento relacionado for encontrado, a função retorna um dicionário indicando que a resposta é \"Não sei\", uma lista vazia de citações e contexto_encontrado como False.\n",
        "answer = document_chain.invoke({\"input\": pergunta, \"context\": docs_relacionados}): Se documentos relacionados forem encontrados, esta linha executa a parte de \"Generation\" (Geração) do RAG. Ela invoca a document_chain (que combina a pergunta do usuário e os documentos recuperados no prompt e envia para o modelo de linguagem). O dicionário passado como argumento para invoke contém a pergunta do usuário (input) e a lista de docs_relacionados (context). A resposta gerada pelo modelo é armazenada na variável answer.\n",
        "txt = (answer or \"\").strip(): Pega o conteúdo da resposta (answer), converte para string caso não seja e remove espaços em branco do início e do fim.\n",
        "if txt.rstrip(\".!?\") == \"Não sei\":: Verifica se a resposta gerada pelo modelo é \"Não sei\" (removendo pontuações finais como ., ! ou ? para garantir a comparação). Lembre-se que o prompt do sistema instrui o modelo a responder \"Não sei\" se não tiver base suficiente.\n",
        "return {\"answer\": \"Não sei.\", \"citacoes\": [], \"contexto_encontrado\": False}: Se a resposta do modelo for \"Não sei\", a função retorna o mesmo dicionário de \"Não sei\" retornado quando nenhum documento foi encontrado.\n",
        "return {\"answer\": txt, \"citacoes\": formatar_citacoes(docs_relacionados, pergunta), \"contexto_encontrado\": True}: Se o modelo gerou uma resposta diferente de \"Não sei\" (o que implica que ele usou o contexto fornecido), a função retorna um dicionário contendo:\n",
        "\"answer\": A resposta gerada pelo modelo (txt).\n",
        "\"citacoes\": Uma lista de citações formatadas, geradas pela função formatar_citacoes (definida em outra célula) com base nos docs_relacionados e na pergunta original. Isso ajuda a dar transparência sobre de onde a resposta veio.\n",
        "\"contexto_encontrado\": True, indicando que documentos relevantes foram encontrados e usados para gerar a resposta.\n",
        "Em resumo, a função perguntar_politica_RAG é o coração do seu sistema RAG. Ela recebe uma pergunta, busca documentos relevantes na sua base de conhecimento, usa esses documentos como contexto para o modelo de linguagem gerar uma resposta e formata a saída, incluindo citações, se um contexto relevante foi encontrado."
      ],
      "metadata": {
        "id": "f8tDceb-jth1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "xsMGEdDMZlol"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "    resposta = perguntar_politica_RAG(msg_teste)\n",
        "    print(f\"PERGUNTA: {msg_teste}\")\n",
        "    print(f\"RESPOSTA: {resposta['answer']}\")\n",
        "    if resposta['contexto_encontrado']:\n",
        "        print(\"CITAÇÕES:\")\n",
        "        for c in resposta['citacoes']:\n",
        "            print(f\" - Documento: {c['documento']}, Página: {c['pagina']}\")\n",
        "            print(f\"   Trecho: {c['trecho']}\")\n",
        "        print(\"------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lxT7ELFZpxl",
        "outputId": "5ab3dc42-1675-4e33-b8ea-5a6dfae7f4b4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERGUNTA: Posso reembolsar a internet?\n",
            "RESPOSTA: Sim, a internet para home office é reembolsável via subsídio mensal de até R$ 100, mediante nota fiscal nominal.\n",
            "CITAÇÕES:\n",
            " - Documento: Política de Reembolsos (Viagens e Despesas).pdf, Página: 1\n",
            "   Trecho: lsáveis.​ 3.​ Transporte: táxi/app são permitidos quando não houver alternativa viável. Comprovantes obrigatórios.​ 4.​ Internet para home office: reembolsável via subsídio mensal de até R$ 100, conforme política de Home Office.​\n",
            " - Documento: Políticas de Home Office.pdf, Página: 1\n",
            "   Trecho: 5.​ Conectividade: há subsídio mensal de internet domiciliar para quem trabalha em home office: até R$ 100/mês, mediante nota fiscal nominal.​ 6.​ Solicitação de\n",
            "------------------------------------\n",
            "PERGUNTA: Quero mais 5 dias de trabalho remoto. Como faço?\n",
            "RESPOSTA: Para solicitar 5 dias de trabalho remoto, você deve formalizar a solicitação via chamado ao RH, incluindo a justificativa do seu gestor.\n",
            "CITAÇÕES:\n",
            " - Documento: Políticas de Home Office.pdf, Página: 1\n",
            "   Trecho:  para quem trabalha em home office: até R$ 100/mês, mediante nota fiscal nominal.​ 6.​ Solicitação de exceção (ex.: 4-5 dias remotos): deve ser formalizada via chamado ao RH com justificativa do gestor.\n",
            " - Documento: Política de Reembolsos (Viagens e Despesas).pdf, Página: 1\n",
            "   Trecho: são reembolsáveis.​ 3.​ Transporte: táxi/app são permitidos quando não houver alternativa viável. Comprovantes obrigatór\n",
            "------------------------------------\n",
            "PERGUNTA: Posso reembolsar cursos ou treinamentos da Alura?\n",
            "RESPOSTA: Sim, cursos e certificações são reembolsáveis, desde que haja aprovação prévia do gestor e orçamento do time.\n",
            "CITAÇÕES:\n",
            " - Documento: Política de Reembolsos (Viagens e Despesas).pdf, Página: 1\n",
            "   Trecho: Política de Reembolsos (Viagens e Despesas) 1.​ Reembolso: requer nota fiscal e deve ser submetido em até 10 dias corrid\n",
            " - Documento: Políticas de Home Office.pdf, Página: 1\n",
            "   Trecho: Políticas de Home Office 1.​ A empresa adota modelo híbrido: mínimo de 2 dias presenciais por semana, salvo exceções apr\n",
            "------------------------------------\n",
            "PERGUNTA: Quantas capivaras tem no Rio Pinheiros?\n",
            "RESPOSTA: Não sei.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código itera sobre uma lista de mensagens de teste e utiliza a função perguntar_politica_RAG para obter a resposta do sistema RAG para cada pergunta. Em seguida, ele imprime a pergunta original, a resposta gerada pelo sistema e, se houver contexto relevante encontrado, imprime as citações de onde a resposta foi extraída.\n",
        "\n",
        "for msg_teste in testes:: Inicia um loop for que irá percorrer cada item na lista chamada testes. Em cada iteração do loop, o item atual da lista será atribuído à variável msg_teste.\n",
        "resposta = perguntar_politica_RAG(msg_teste): Chama a função perguntar_politica_RAG (que definimos anteriormente) passando a mensagem de teste atual (msg_teste) como argumento. Esta função executa todo o processo de RAG (recuperação de documentos relevantes, envio para o modelo de linguagem com contexto) e retorna um dicionário contendo a resposta, as citações e um indicador se o contexto foi encontrado. O resultado é armazenado na variável resposta.\n",
        "print(f\"PERGUNTA: {msg_teste}\"): Imprime a pergunta de teste atual.\n",
        "print(f\"RESPOSTA: {resposta['answer']}\"): Imprime a resposta gerada pelo sistema RAG, acessando o valor associado à chave 'answer' no dicionário resposta.\n",
        "if resposta['contexto_encontrado']:: Verifica se o valor associado à chave 'contexto_encontrado' no dicionário resposta é True. Isso indica que o sistema encontrou documentos relevantes e usou-os como contexto para gerar a resposta.\n",
        "print(\"CITAÇÕES:\"): Se contexto_encontrado for True, imprime o cabeçalho \"CITAÇÕES:\".\n",
        "for c in resposta['citacoes']:: Inicia um loop aninhado que itera sobre a lista de citações armazenada na chave 'citacoes' do dicionário resposta. Cada item nesta lista (c) é um dicionário contendo informações sobre a citação.\n",
        "print(f\" - Documento: {c['documento']}, Página: {c['pagina']}\"): Dentro do loop de citações, imprime o nome do documento e o número da página de onde o trecho foi extraído.\n",
        "print(f\" Trecho: {c['trecho']}\"): Imprime o trecho específico do documento que foi recuperado como parte da citação.\n",
        "print(\"------------------------------------\"): Imprime uma linha separadora após as citações de cada pergunta (se houver) para melhorar a legibilidade da saída.\n",
        "Em resumo, este código demonstra como usar a função perguntar_politica_RAG com uma lista de perguntas de teste e exibe os resultados de forma estruturada, incluindo as citações relevantes quando o sistema consegue encontrar informações na base de conhecimento."
      ],
      "metadata": {
        "id": "5PUmNSadk0lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AULA 03 = ORQUESTRAÇÃO COM AGENTE LANGGRAPH"
      ],
      "metadata": {
        "id": "RBmT64D21aZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "iNSTALAR o Langgraph"
      ],
      "metadata": {
        "id": "lES7_cHt1f6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWI08ZjpZtRN",
        "outputId": "1cf2ed5c-982a-42ed-b5aa-995acfd73b98"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como montar a logicas dos grafos,  quem é o agente"
      ],
      "metadata": {
        "id": "3QakDmj52WIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Optional\n",
        "\n",
        "class AgentState(TypedDict, total = False):\n",
        "    pergunta: str\n",
        "    triagem: dict\n",
        "    resposta: Optional[str]\n",
        "    citacoes: List[dict]\n",
        "    rag_sucesso: bool\n",
        "    acao_final: str"
      ],
      "metadata": {
        "id": "mO1m6w6c18ek"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código define uma estrutura de dados chamada AgentState usando TypedDict do módulo typing. Essa estrutura é usada para representar o estado de um agente em um workflow, como os construídos com a biblioteca langgraph.\n",
        "\n",
        "from typing import TypedDict, Optional, List: Importa os tipos TypedDict, Optional e List do módulo typing.\n",
        "\n",
        "TypedDict: Permite definir um tipo de dicionário onde você especifica os nomes e os tipos dos campos esperados. Isso fornece tipagem estática e clareza sobre a estrutura dos dados.\n",
        "\n",
        "Optional: Indica que um campo pode ter o tipo especificado ou ser None.\n",
        "\n",
        "List: Indica que um campo é uma lista de itens de um tipo especificado.\n",
        "\n",
        "class AgentState(TypedDict, total = False):: Define uma nova classe AgentState que herda de TypedDict.\n",
        "\n",
        "total = False: Este argumento opcional significa que nem todos os campos listados na definição da classe são obrigatórios. Um dicionário do tipo AgentState pode não conter todos esses campos.\n",
        "\n",
        "pergunta: str: Define um campo chamado pergunta que deve ser uma string (str).\n",
        "triagem: dict: Define um campo chamado triagem que deve ser um dicionário (dict).\n",
        "\n",
        "resposta: Optional[str]: Define um campo chamado resposta que pode ser uma string (str) ou None.\n",
        "\n",
        "citacoes: List[dict]: Define um campo chamado citacoes que deve ser uma lista (List) de dicionários (dict).\n",
        "\n",
        "rag_sucesso: bool: Define um campo chamado rag_sucesso que deve ser um booleano (bool), indicando se o processo RAG foi bem-sucedido.\n",
        "\n",
        "acao_final: str: Define um campo chamado acao_final que deve ser uma string (str), representando a ação final tomada pelo agente.\n",
        "\n",
        "Em resumo, esta classe AgentState define um esquema para um dicionário que manterá o estado durante a execução de um agente. Ela especifica que o estado conterá a pergunta original, o resultado da triagem, a resposta gerada pelo RAG (opcional), as citações relevantes, um indicador de sucesso do RAG e a ação final decidida. O uso de TypedDict ajuda a garantir que os dados passados entre as diferentes etapas do agente sigam uma estrutura consistente."
      ],
      "metadata": {
        "id": "jtLITev_2DEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "criar os codigos a partir do diagrama criado, nó de triagem"
      ],
      "metadata": {
        "id": "FJnKuPmb9TY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_triagem(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de triagem...\")\n",
        "    return {\"triagem\": triagem(state[\"pergunta\"])}"
      ],
      "metadata": {
        "id": "sog5D1iD8pOs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código selecionado define uma função chamada node_triagem, que representa um nó no seu fluxo de trabalho LangGraph.\n",
        "\n",
        "Aqui está a análise:\n",
        "\n",
        "def node_triagem(state: AgentState) -> AgentState:: Esta linha define a função node_triagem.\n",
        "state: AgentState indica que a função espera um argumento chamado state, que deve estar em conformidade com o TypedDict AgentState que você definiu anteriormente. Este dicionário state armazena os dados atuais e o progresso do fluxo de trabalho do seu agente.\n",
        "-> AgentState indica que a função deve retornar um dicionário que também esteja em conformidade com o tipo AgentState.\n",
        "print(\"Executando nó de triagem...\"): Esta linha simplesmente imprime uma mensagem no console indicando que este nó específico no grafo está sendo executado. Isso é útil para depuração e rastreamento do fluxo do agente.\n",
        "return {\"triagem\": triagem(state[\"pergunta\"])}: Esta é a lógica central deste nó.\n",
        "state[\"pergunta\"]: Ele acessa o valor associado à chave \"pergunta\" no dicionário state de entrada. Esta pergunta é a mensagem ou consulta original do usuário.\n",
        "triagem(state[\"pergunta\"]): Ele chama a função triagem (que você definiu anteriormente para interagir com o LLM de triagem) e passa a pergunta do usuário para ela. A função triagem é responsável por classificar a consulta do usuário (por exemplo, AUTO_RESOLVER, PEDIR_INFO, ABRIR_CHAMADO).\n",
        "{\"triagem\": ...}: Cria um novo dicionário onde a chave é \"triagem\" e o valor é o resultado retornado pela função triagem().\n",
        "return ...: Este novo dicionário contendo o resultado da triagem é retornado. O LangGraph então atualizará o state geral do agente com este valor retornado.\n",
        "Em essência, esta função node_triagem recebe o estado atual do agente (especificamente, a pergunta do usuário), usa sua função triagem para classificar a pergunta e, em seguida, atualiza o estado do agente adicionando o resultado da triagem sob a chave \"triagem\". Este estado atualizado é então passado para o próximo nó no seu fluxo de trabalho LangGraph com base nas arestas condicionais que você definiu."
      ],
      "metadata": {
        "id": "8Mz7_rfIqsR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nó do auto_resolver"
      ],
      "metadata": {
        "id": "UFJ2BJKp6kZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_auto_resolver(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de auto_resolver...\")\n",
        "    resposta_rag = perguntar_politica_RAG(state[\"pergunta\"])\n",
        "\n",
        "    update: AgentState = {\n",
        "        \"resposta\": resposta_rag[\"answer\"],\n",
        "        \"citacoes\": resposta_rag.get(\"citacoes\", []),\n",
        "        \"rag_sucesso\": resposta_rag[\"contexto_encontrado\"],\n",
        "    }\n",
        "\n",
        "    if resposta_rag[\"contexto_encontrado\"]:\n",
        "        update[\"acao_final\"] = \"AUTO_RESOLVER\"\n",
        "\n",
        "    return update"
      ],
      "metadata": {
        "id": "MyFZOyQY8qDW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " O código na célula MyFZOyQY8qDW define a função node_auto_resolver, que é outro nó no seu fluxo de trabalho LangGraph.\n",
        "\n",
        "Veja a análise detalhada:\n",
        "\n",
        "def node_auto_resolver(state: AgentState) -> AgentState:: Define a função node_auto_resolver. Assim como o nó de triagem, ela recebe o estado atual do agente (state do tipo AgentState) e deve retornar um estado atualizado do mesmo tipo.\n",
        "print(\"Executando nó de auto_resolver...\"): Imprime uma mensagem indicando que este nó está sendo executado.\n",
        "resposta_rag = perguntar_politica_RAG(state[\"pergunta\"]): Esta é a parte crucial. Ele chama a função perguntar_politica_RAG (que implementa a lógica do seu sistema RAG) passando a pergunta original do usuário (state[\"pergunta\"]). O resultado (que inclui a resposta gerada pelo RAG, as citações e um indicador se o contexto foi encontrado) é armazenado em resposta_rag.\n",
        "update: AgentState = { ... }: Cria um dicionário update que será usado para atualizar o estado do agente.\n",
        "\"resposta\": resposta_rag[\"answer\"]: Adiciona a resposta gerada pelo sistema RAG ao estado sob a chave \"resposta\".\n",
        "\"citacoes\": resposta_rag.get(\"citacoes\", []): Adiciona a lista de citações retornada pelo RAG ao estado sob a chave \"citacoes\". Se não houver citações (por exemplo, se o RAG não encontrou contexto), ele usa uma lista vazia [] como padrão (.get(\"citacoes\", [])).\n",
        "\"rag_sucesso\": resposta_rag[\"contexto_encontrado\"]: Adiciona o indicador booleano de sucesso do RAG (True se encontrou contexto e gerou resposta, False caso contrário) ao estado sob a chave \"rag_sucesso\".\n",
        "if resposta_rag[\"contexto_encontrado\"]:: Verifica se o sistema RAG conseguiu encontrar contexto relevante (\"contexto_encontrado\" é True).\n",
        "update[\"acao_final\"] = \"AUTO_RESOLVER\": Se o RAG foi bem-sucedido em encontrar contexto e gerar uma resposta, a ação final é definida como \"AUTO_RESOLVER\". Isso significa que o nó auto_resolver conseguiu lidar com a pergunta diretamente usando a base de conhecimento.\n",
        "return update: Retorna o dicionário update. O LangGraph usará este dicionário para mesclar as informações com o estado atual do agente, efetivamente atualizando-o com a resposta, citações, indicador de sucesso do RAG e a ação final decidida neste nó.\n",
        "Em suma, a função node_auto_resolver tenta usar o seu sistema RAG para responder à pergunta do usuário. Se o RAG encontrar informações relevantes e gerar uma resposta, ele atualiza o estado com essa resposta, as citações e marca a ação final como AUTO_RESOLVER. Este estado atualizado será então usado pelas arestas condicionais do grafo para determinar o próximo passo."
      ],
      "metadata": {
        "id": "4B-NF60_60ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nó de pedir informação"
      ],
      "metadata": {
        "id": "19PRfH_V8RBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_pedir_info(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de pedir_info...\")\n",
        "    faltantes = state[\"triagem\"].get(\"campos_faltantes\", [])\n",
        "    if faltantes:\n",
        "        detalhe = \",\".join(faltantes)\n",
        "    else:\n",
        "        detalhe = \"Tema e contexto específico\"\n",
        "\n",
        "    return {\n",
        "        \"resposta\": f\"Para avançar, preciso que detalhe: {detalhe}\",\n",
        "        \"citacoes\": [],\n",
        "        \"acao_final\": \"PEDIR_INFO\"\n",
        "    }"
      ],
      "metadata": {
        "id": "fytM9hmt8s-r"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código define a função node_pedir_info, que é um nó no seu fluxo de trabalho LangGraph responsável por lidar com situações em que a triagem inicial indicou que mais informações são necessárias do usuário.\n",
        "\n",
        "Aqui está a análise:\n",
        "\n",
        "def node_pedir_info(state: AgentState) -> AgentState:: Define a função node_pedir_info. Ela recebe o estado atual do agente (state do tipo AgentState) e retorna um estado atualizado do mesmo tipo.\n",
        "print(\"Executando nó de pedir_info...\"): Imprime uma mensagem para indicar que este nó está sendo executado.\n",
        "faltantes = state[\"triagem\"].get(\"campos_faltantes\", []): Acessa o resultado da triagem (state[\"triagem\"]) e tenta obter a lista de campos faltantes usando .get(\"campos_faltantes\", []). O .get() é usado para garantir que se a chave \"campos_faltantes\" não existir no dicionário de triagem, ele retorne uma lista vazia [] em vez de gerar um erro.\n",
        "if faltantes: detalhe = \",\".join(faltantes): Verifica se a lista faltantes não está vazia. Se houver campos faltantes listados na triagem, ele junta esses nomes de campos em uma única string separada por vírgulas (por exemplo, \"nome,email,detalhes\").\n",
        "else: detalhe = \"Tema e contexto específico\": Se a lista faltantes estiver vazia (o que pode acontecer se a triagem simplesmente não conseguiu entender o suficiente para identificar campos específicos), a variável detalhe é definida com a string genérica \"Tema e contexto específico\".\n",
        "return { ... }: Retorna um dicionário para atualizar o estado do agente.\n",
        "\"resposta\": f\"Para avançar, preciso que detalhe: {detalhe}\": Define a chave \"resposta\" no estado como uma mensagem formatada que pede ao usuário para fornecer detalhes. A string formatada inclui o conteúdo da variável detalhe, que será a lista de campos faltantes ou a mensagem genérica.\n",
        "\"citacoes\": []: Define a chave \"citacoes\" como uma lista vazia. Neste nó, não há uma resposta baseada em documentos, então não há citações para fornecer.\n",
        "\"acao_final\": \"PEDIR_INFO\": Define a chave \"acao_final\" como \"PEDIR_INFO\", indicando que a ação final tomada pelo agente neste ponto do fluxo foi solicitar mais informações ao usuário.\n",
        "Em resumo, a função node_pedir_info é acionada quando a triagem determina que a pergunta do usuário é vaga. Ela verifica se a triagem conseguiu identificar quais informações específicas estão faltando. Em seguida, constrói uma mensagem solicitando esses detalhes ao usuário (ou uma mensagem mais geral se os campos faltantes não foram identificados) e atualiza o estado do agente com esta mensagem e a ação final \"PEDIR_INFO\"."
      ],
      "metadata": {
        "id": "I3p1Co7m8bqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nó de abrir Chamado"
      ],
      "metadata": {
        "id": "hevAtmbj-ZSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_abrir_chamado(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de abrir_chamado...\")\n",
        "    triagem = state[\"triagem\"]\n",
        "\n",
        "    return {\n",
        "        \"resposta\": f\"Abrindo chamado com urgência {triagem['urgencia']}. Descrição: {state['pergunta'][:140]}\",\n",
        "        \"citacoes\": [],\n",
        "        \"acao_final\": \"ABRIR_CHAMADO\"\n",
        "    }"
      ],
      "metadata": {
        "id": "Bd2yndsC8wWs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código define a função node_abrir_chamado, que é um nó no seu fluxo de trabalho LangGraph responsável por lidar com situações em que a triagem inicial indicou que um chamado deve ser aberto.\n",
        "\n",
        "Aqui está a análise:\n",
        "\n",
        "def node_abrir_chamado(state: AgentState) -> AgentState:: Define a função node_abrir_chamado. Ela recebe o estado atual do agente (state do tipo AgentState) e retorna um estado atualizado do mesmo tipo.\n",
        "\n",
        "print(\"Executando nó de abrir_chamado...\"): Imprime uma mensagem para indicar que este nó está sendo executado.\n",
        "\n",
        "triagem = state[\"triagem\"]: Acessa o resultado da triagem armazenado no estado sob a chave \"triagem\" e o armazena na variável local triagem para facilitar o acesso.\n",
        "\n",
        "return { ... }: Retorna um dicionário para atualizar o estado do agente.\n",
        "\"resposta\": f\"Abrindo chamado com urgência {triagem['urgencia']}. Descrição: {state['pergunta'][:140]}\": Define a chave \"resposta\" no estado como uma mensagem formatada que informa ao usuário que um chamado está sendo aberto.\n",
        "triagem['urgencia']: Acessa o nível de urgência determinado pelo nó de triagem.\n",
        "\n",
        "state['pergunta'][:140]: Pega os primeiros 140 caracteres da pergunta original do usuário para usar como uma breve descrição do chamado.\n",
        "\"citacoes\": []: Define a chave \"citacoes\" como uma lista vazia. Neste nó, não há uma resposta baseada em documentos, então não há citações para fornecer.\n",
        "\"acao_final\": \"ABRIR_CHAMADO\": Define a chave \"acao_final\" como \"ABRIR_CHAMADO\", indicando que a ação final tomada pelo agente neste ponto do fluxo foi abrir um chamado.\n",
        "Em resumo, a função node_abrir_chamado é acionada quando a triagem determina que a solicitação do usuário requer a abertura de um chamado. Ela constrói uma mensagem informativa para o usuário, incluindo a urgência determinada pela triagem e uma breve descrição baseada na pergunta original, e atualiza o estado do agente com esta mensagem e a ação final \"ABRIR_CHAMADO\"."
      ],
      "metadata": {
        "id": "j_Jz9dDo-opI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lista de palavras chaves como camada de segurança e ajudar a direcionar a abertura do chamdo"
      ],
      "metadata": {
        "id": "DmxXwIHzAGjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORDS_ABRIR_TICKET = [\"aprovação\", \"exceção\", \"liberação\", \"abrir ticket\", \"abrir chamado\", \"acesso especial\"]\n",
        "\n",
        "def decidir_pos_triagem(state: AgentState) -> str:\n",
        "    print(\"Decidindo após a triagem...\")\n",
        "    decisao = state[\"triagem\"][\"decisao\"]\n",
        "\n",
        "    if decisao == \"AUTO_RESOLVER\": return \"auto\"\n",
        "    if decisao == \"PEDIR_INFO\": return \"info\"\n",
        "    if decisao == \"ABRIR_CHAMADO\": return \"chamado\""
      ],
      "metadata": {
        "id": "2NrhvMBw8z2c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código define uma lista de palavras-chave e uma função que decide o próximo passo no fluxo do LangGraph após a triagem inicial.\n",
        "\n",
        "Aqui está a análise:\n",
        "\n",
        "KEYWORDS_ABRIR_TICKET = [\"aprovação\", \"exceção\", \"liberação\", \"abrir ticket\", \"abrir chamado\", \"acesso especial\"]:\n",
        "Esta linha define uma lista constante chamada KEYWORDS_ABRIR_TICKET.\n",
        "Ela contém uma lista de strings que representam palavras-chave ou frases que, se encontradas na pergunta do usuário, sugerem fortemente que a intenção do usuário é solicitar algo que provavelmente exigirá a abertura de um chamado (ticket) em vez de ser resolvido automaticamente.\n",
        "Esta lista atua como uma camada de segurança ou um mecanismo de direcionamento para ajudar a garantir que solicitações que requerem ação humana ou exceções sejam encaminhadas corretamente, mesmo que a triagem inicial possa ter classificado de outra forma (embora neste grafo específico, a triagem já lida com \"ABRIR_CHAMADO\"). No entanto, esta lista é mais relevante para a função decidir_pos_auto_resolver na próxima célula.\n",
        "def decidir_pos_triagem(state: AgentState) -> str::\n",
        "Define uma função chamada decidir_pos_triagem. Esta função é um \"nó condicional\" no LangGraph. Ela recebe o estado atual do agente (state) e retorna uma string que o LangGraph usa para determinar qual aresta seguir (ou seja, para qual nó ir em seguida).\n",
        "print(\"Decidindo após a triagem...\"): Imprime uma mensagem indicando que esta função está sendo executada.\n",
        "decisao = state[\"triagem\"][\"decisao\"]: Acessa o resultado da triagem (que foi adicionado ao estado pelo nó node_triagem) e extrai o valor da chave \"decisao\" (que será \"AUTO_RESOLVER\", \"PEDIR_INFO\" ou \"ABRIR_CHAMADO\").\n",
        "if decisao == \"AUTO_RESOLVER\": return \"auto\": Se a decisão da triagem for \"AUTO_RESOLVER\", a função retorna a string \"auto\". O LangGraph usará isso para seguir a aresta rotulada como \"auto\".\n",
        "if decisao == \"PEDIR_INFO\": return \"info\": Se a decisão da triagem for \"PEDIR_INFO\", a função retorna a string \"info\". O LangGraph usará isso para seguir a aresta rotulada como \"info\".\n",
        "if decisao == \"ABRIR_CHAMADO\": return \"chamado\": Se a decisão da triagem for \"ABRIR_CHAMADO\", a função retorna a string \"chamado\". O LangGraph usará isso para seguir a aresta rotulada como \"chamado\".\n",
        "Em resumo, a lista KEYWORDS_ABRIR_TICKET fornece um conjunto de termos que podem ser usados para identificar intenções de abrir chamado, enquanto a função decidir_pos_triagem atua como um roteador após o nó de triagem, direcionando o fluxo do agente para o nó apropriado (auto_resolver, pedir_info ou abrir_chamado) com base no resultado da triagem."
      ],
      "metadata": {
        "id": "khRC7AjsBCY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nó para abrir uma funçaõ depois do auto resolver"
      ],
      "metadata": {
        "id": "abQczKBJCDbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decidir_pos_auto_resolver(state: AgentState) -> str:\n",
        "    print(\"Decidindo após o auto_resolver...\")\n",
        "\n",
        "    if state.get(\"rag_sucesso\"):\n",
        "        print(\"Rag com sucesso, finalizando o fluxo.\")\n",
        "        return \"ok\"\n",
        "\n",
        "    state_da_pergunta = (state[\"pergunta\"] or \"\").lower()\n",
        "\n",
        "    if any(k in state_da_pergunta for k in KEYWORDS_ABRIR_TICKET):\n",
        "        print(\"Rag falhou, mas foram encontradas keywords de abertura de ticket. Abrindo...\")\n",
        "        return \"chamado\"\n",
        "\n",
        "    print(\"Rag falhou, sem keywords, vou pedir mais informações...\")\n",
        "    return \"info\""
      ],
      "metadata": {
        "id": "rNNfiQNG83R5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Este código define a função decidir_pos_auto_resolver, que é um nó condicional no seu fluxo de trabalho LangGraph. Ele é responsável por determinar o próximo passo após a execução do nó auto_resolver.\n",
        "\n",
        "Aqui está a análise detalhada:\n",
        "\n",
        "def decidir_pos_auto_resolver(state: AgentState) -> str:: Define a função decidir_pos_auto_resolver. Ela recebe o estado atual do agente (state do tipo AgentState) e retorna uma string que o LangGraph usa para determinar qual aresta seguir.\n",
        "print(\"Decidindo após o auto_resolver...\"): Imprime uma mensagem indicando que esta função condicional está sendo executada.\n",
        "if state.get(\"rag_sucesso\"):: Verifica se o sistema RAG foi bem-sucedido em encontrar contexto e gerar uma resposta. Ele acessa o valor da chave \"rag_sucesso\" no estado (que foi definido pelo nó node_auto_resolver). .get(\"rag_sucesso\") é usado para evitar erros caso a chave não exista no estado por algum motivo. Se rag_sucesso for True, significa que o RAG encontrou uma resposta relevante.\n",
        "print(\"Rag com sucesso, finalizando o fluxo.\"): Imprime uma mensagem indicando que o RAG foi bem-sucedido.\n",
        "return \"ok\": Se o RAG foi bem-sucedido, a função retorna a string \"ok\". No grafo, a aresta com o rótulo \"ok\" leva ao nó END, finalizando o fluxo do agente.\n",
        "state_da_pergunta = (state[\"pergunta\"] or \"\").lower(): Se o RAG não foi bem-sucedido (a condição anterior foi falsa), esta linha acessa a pergunta original do usuário (state[\"pergunta\"]), garante que é uma string (usando or \"\") e a converte para minúsculas. Isso é feito para uma comparação de palavras-chave case-insensitive.\n",
        "if any(k in state_da_pergunta for k in KEYWORDS_ABRIR_TICKET):: Verifica se alguma das palavras-chave na lista KEYWORDS_ABRIR_TICKET (definida em uma célula anterior) está presente na pergunta do usuário em minúsculas. any() retorna True se pelo menos uma palavra-chave for encontrada.\n",
        "print(\"Rag falhou, mas foram encontradas keywords de abertura de ticket. Abrindo...\"): Imprime uma mensagem indicando que o RAG falhou, mas palavras-chave relacionadas à abertura de chamado foram encontradas.\n",
        "return \"chamado\": Se palavras-chave de abertura de chamado forem encontradas, a função retorna a string \"chamado\". No grafo, a aresta com o rótulo \"chamado\" leva ao nó abrir_chamado. Isso funciona como um fallback: se o RAG não conseguiu responder diretamente, mas a pergunta sugere a necessidade de um chamado, o agente encaminha para o nó apropriado.\n",
        "print(\"Rag falhou, sem keywords, vou pedir mais informações...\"): Se o RAG falhou e nenhuma palavra-chave de abertura de chamado foi encontrada, imprime uma mensagem indicando que mais informações serão solicitadas.\n",
        "return \"info\": Neste caso, a função retorna a string \"info\". No grafo, a aresta com o rótulo \"info\" leva ao nó pedir_info, solicitando ao usuário que forneça mais detalhes.\n",
        "Em resumo, a função decidir_pos_auto_resolver verifica o resultado da tentativa do auto_resolver de responder usando o RAG. Se o RAG foi bem-sucedido, o fluxo termina. Se não, ela verifica a pergunta original por palavras-chave que sugiram a necessidade de abrir um chamado. Se encontrar, direciona para o nó de abertura de chamado; caso contrário, direciona para o nó que solicita mais informações ao usuário."
      ],
      "metadata": {
        "id": "Y_Tp4_F8CzQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar a conexao com o langgraph"
      ],
      "metadata": {
        "id": "_e3of8N0HObw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"triagem\", node_triagem)\n",
        "workflow.add_node(\"auto_resolver\", node_auto_resolver)\n",
        "workflow.add_node(\"pedir_info\", node_pedir_info)\n",
        "workflow.add_node(\"abrir_chamado\", node_abrir_chamado)\n",
        "\n",
        "workflow.add_edge(START, \"triagem\")\n",
        "workflow.add_conditional_edges(\"triagem\", decidir_pos_triagem, {\n",
        "    \"auto\": \"auto_resolver\",\n",
        "    \"info\": \"pedir_info\",\n",
        "    \"chamado\": \"abrir_chamado\"\n",
        "})\n",
        "\n",
        "workflow.add_conditional_edges(\"auto_resolver\", decidir_pos_auto_resolver, {\n",
        "    \"info\": \"pedir_info\",\n",
        "    \"chamado\": \"abrir_chamado\",\n",
        "    \"ok\": END\n",
        "})\n",
        "\n",
        "workflow.add_edge(\"pedir_info\", END)\n",
        "workflow.add_edge(\"abrir_chamado\", END)\n",
        "\n",
        "grafo = workflow.compile()"
      ],
      "metadata": {
        "id": "-G16lnoP861U"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Este trecho de código configura o grafo do LangGraph, definindo os nós e as transições entre eles para orquestrar o fluxo do seu agente de atendimento.\n",
        "\n",
        "Aqui está a análise detalhada:\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END: Importa as classes necessárias do módulo langgraph.graph.\n",
        "StateGraph: É a classe principal para definir um grafo que gerencia um estado compartilhado entre os nós.\n",
        "START: Um ponto especial para indicar o início do grafo.\n",
        "END: Um ponto especial para indicar o fim do grafo.\n",
        "workflow = StateGraph(AgentState): Cria uma instância de StateGraph chamada workflow. O AgentState (o TypedDict que você definiu anteriormente) é passado como argumento para indicar que este grafo usará um estado com essa estrutura.\n",
        "workflow.add_node(\"triagem\", node_triagem): Adiciona um nó ao grafo.\n",
        "\"triagem\": É o nome que você dá a este nó.\n",
        "node_triagem: É a função Python que será executada quando o fluxo atingir este nó.\n",
        "workflow.add_node(\"auto_resolver\", node_auto_resolver): Adiciona o nó \"auto_resolver\", associado à função node_auto_resolver.\n",
        "workflow.add_node(\"pedir_info\", node_pedir_info): Adiciona o nó \"pedir_info\", associado à função node_pedir_info.\n",
        "workflow.add_node(\"abrir_chamado\", node_abrir_chamado): Adiciona o nó \"abrir_chamado\", associado à função node_abrir_chamado.\n",
        "workflow.add_edge(START, \"triagem\"): Define uma aresta direta (sem condição) do ponto de início (START) para o nó \"triagem\". Isso significa que o fluxo sempre começará executando o nó \"triagem\".\n",
        "workflow.add_conditional_edges(\"triagem\", decidir_pos_triagem, { ... }): Define arestas condicionais a partir do nó \"triagem\".\n",
        "\"triagem\": O nó de origem das arestas condicionais.\n",
        "decidir_pos_triagem: A função condicional que será executada após o nó \"triagem\". Esta função retorna uma string (\"auto\", \"info\" ou \"chamado\") que determina para onde o fluxo irá.\n",
        "{ \"auto\": \"auto_resolver\", \"info\": \"pedir_info\", \"chamado\": \"abrir_chamado\" }: Um dicionário que mapeia as strings retornadas pela função condicional para os nomes dos próximos nós. Se decidir_pos_triagem retornar \"auto\", o fluxo vai para \"auto_resolver\"; se retornar \"info\", vai para \"pedir_info\"; se retornar \"chamado\", vai para \"abrir_chamado\".\n",
        "workflow.add_conditional_edges(\"auto_resolver\", decidir_pos_auto_resolver, { ... }): Define arestas condicionais a partir do nó \"auto_resolver\".\n",
        "\"auto_resolver\": O nó de origem.\n",
        "decidir_pos_auto_resolver: A função condicional que decide o próximo passo após o auto_resolver.\n",
        "{ \"info\": \"pedir_info\", \"chamado\": \"abrir_chamado\", \"ok\": END }: Mapeia as saídas de decidir_pos_auto_resolver. Se retornar \"info\", vai para \"pedir_info\"; se retornar \"chamado\", vai para \"abrir_chamado\"; se retornar \"ok\", o fluxo termina (END).\n",
        "workflow.add_edge(\"pedir_info\", END): Adiciona uma aresta direta do nó \"pedir_info\" para o ponto de fim (END). Depois de pedir informações, o fluxo termina.\n",
        "workflow.add_edge(\"abrir_chamado\", END): Adiciona uma aresta direta do nó \"abrir_chamado\" para o ponto de fim (END). Depois de simular a abertura do chamado, o fluxo termina.\n",
        "grafo = workflow.compile(): Compila o fluxo de trabalho definido no workflow em um objeto executável chamado grafo. Este objeto grafo pode então ser invocado para executar o fluxo do agente com uma entrada inicial.\n",
        "Em resumo, este código constrói a estrutura do seu agente LangGraph, definindo os diferentes estágios (nós como triagem, auto_resolver, pedir_info, abrir_chamado) e as regras de como o fluxo de execução se move entre esses estágios com base em decisões condicionais ou caminhos diretos, começando no START e terminando no END. O objeto compilado grafo é o que você usa para realmente executar o agente."
      ],
      "metadata": {
        "id": "Cos9Ih_HHoLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizção das conexoes"
      ],
      "metadata": {
        "id": "FUEGqOa4eSXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "\n",
        "graph_bytes = grafo.get_graph().draw_mermaid_png()\n",
        "display(Image(graph_bytes))"
      ],
      "metadata": {
        "id": "Kdzs_7fU8_zz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "598a4b71-d20e-44e9-f194-a0a1f86b8e83"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Este código é utilizado para visualizar o fluxo (grafo) que você construiu usando a biblioteca LangGraph. Ele gera uma imagem do grafo para que você possa ver as conexões entre os nós e entender o caminho que a execução do agente pode seguir.\n",
        "\n",
        "Aqui está a análise:\n",
        "\n",
        "from IPython.display import display, Image: Importa as classes display e Image do módulo IPython.display. Essas classes são usadas no ambiente do Colab para exibir imagens diretamente na saída da célula.\n",
        "\n",
        "graph_bytes = grafo.get_graph().draw_mermaid_png(): Esta linha obtém a representação do seu grafo e a converte em um formato visual.\n",
        "\n",
        "grafo.get_graph(): Obtém o objeto grafo interno do LangGraph que descreve a estrutura do fluxo de trabalho que você definiu.\n",
        "\n",
        ".draw_mermaid_png(): Este método tenta renderizar o grafo em formato Mermaid (uma sintaxe para diagramas e fluxogramas) e, em seguida, gera uma imagem PNG desses diagramas. O resultado é um objeto bytes que representa a imagem PNG.\n",
        "display(Image(graph_bytes)): Esta linha exibe a imagem gerada na saída da célula.\n",
        "\n",
        "Image(graph_bytes): Cria um objeto Image a partir dos bytes da imagem PNG gerada.\n",
        "\n",
        "display(...): A função display do IPython renderiza este objeto Image para visualização no notebook.\n",
        "\n",
        "Em resumo, este código pega a definição estrutural do seu agente LangGraph, a transforma em um diagrama visual usando a sintaxe Mermaid e exibe esse diagrama como uma imagem PNG diretamente no seu notebook Colab. Isso é extremamente útil para depurar e entender a lógica do fluxo do seu agente. Lembre-se de corrigir o erro de digitação \"rom\" para \"from\" no comando de importação para que este código funcione corretamente."
      ],
      "metadata": {
        "id": "0_TDGVrEehXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTAR O AGENTE FINAL"
      ],
      "metadata": {
        "id": "pDLory3zgBmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"É possível reembolsar certificações do Google Cloud?\",\n",
        "          \"Posso obter o Google Gemini de graça?\",\n",
        "          \"Qual é a palavra-chave da aula de hoje?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "Hek_lWVm9HUT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CÓDIGO QUE IRÁ RODAR O GRÁFICO"
      ],
      "metadata": {
        "id": "M-qgyAhQgh3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_test in testes:\n",
        "    resposta_final = grafo.invoke({\"pergunta\": msg_test})\n",
        "\n",
        "    triag = resposta_final.get(\"triagem\", {})\n",
        "    print(f\"PERGUNTA: {msg_test}\")\n",
        "    print(f\"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}\")\n",
        "    print(f\"RESPOSTA: {resposta_final.get('resposta')}\")\n",
        "    if resposta_final.get(\"citacoes\"):\n",
        "        print(\"CITAÇÕES:\")\n",
        "        for citacao in resposta_final.get(\"citacoes\"):\n",
        "            print(f\" - Documento: {citacao['documento']}, Página: {citacao['pagina']}\")\n",
        "            print(f\"   Trecho: {citacao['trecho']}\")\n",
        "\n",
        "    print(\"------------------------------------\")"
      ],
      "metadata": {
        "id": "eLp-lqLC9INL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2fa733-e595-4b71-fd7e-84bd80c89cb7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executando nó de triagem...\n",
            "Decidindo após a triagem...\n",
            "Executando nó de auto_resolver...\n",
            "Decidindo após o auto_resolver...\n",
            "Rag com sucesso, finalizando o fluxo.\n",
            "PERGUNTA: Posso reembolsar a internet?\n",
            "DECISÃO: AUTO_RESOLVER | URGÊNCIA: BAIXA | AÇÃO FINAL: AUTO_RESOLVER\n",
            "RESPOSTA: Sim, a internet para home office é reembolsável via subsídio mensal de até R$ 100, mediante nota fiscal nominal.\n",
            "CITAÇÕES:\n",
            " - Documento: Política de Reembolsos (Viagens e Despesas).pdf, Página: 1\n",
            "   Trecho: lsáveis.​ 3.​ Transporte: táxi/app são permitidos quando não houver alternativa viável. Comprovantes obrigatórios.​ 4.​ Internet para home office: reembolsável via subsídio mensal de até R$ 100, conforme política de Home Office.​\n",
            " - Documento: Políticas de Home Office.pdf, Página: 1\n",
            "   Trecho: 5.​ Conectividade: há subsídio mensal de internet domiciliar para quem trabalha em home office: até R$ 100/mês, mediante nota fiscal nominal.​ 6.​ Solicitação de\n",
            "------------------------------------\n",
            "Executando nó de triagem...\n",
            "Decidindo após a triagem...\n",
            "Executando nó de abrir_chamado...\n",
            "PERGUNTA: Quero mais 5 dias de trabalho remoto. Como faço?\n",
            "DECISÃO: ABRIR_CHAMADO | URGÊNCIA: MEDIA | AÇÃO FINAL: ABRIR_CHAMADO\n",
            "RESPOSTA: Abrindo chamado com urgência MEDIA. Descrição: Quero mais 5 dias de trabalho remoto. Como faço?\n",
            "------------------------------------\n",
            "Executando nó de triagem...\n",
            "Decidindo após a triagem...\n",
            "Executando nó de auto_resolver...\n",
            "Decidindo após o auto_resolver...\n",
            "Rag com sucesso, finalizando o fluxo.\n",
            "PERGUNTA: Posso reembolsar cursos ou treinamentos da Alura?\n",
            "DECISÃO: AUTO_RESOLVER | URGÊNCIA: BAIXA | AÇÃO FINAL: AUTO_RESOLVER\n",
            "RESPOSTA: Sim, cursos e certificações são reembolsáveis, desde que haja aprovação prévia do gestor e orçamento do time.\n",
            "CITAÇÕES:\n",
            " - Documento: Política de Reembolsos (Viagens e Despesas).pdf, Página: 1\n",
            "   Trecho: Política de Reembolsos (Viagens e Despesas) 1.​ Reembolso: requer nota fiscal e deve ser submetido em até 10 dias corrid\n",
            " - Documento: Políticas de Home Office.pdf, Página: 1\n",
            "   Trecho: Políticas de Home Office 1.​ A empresa adota modelo híbrido: mínimo de 2 dias presenciais por semana, salvo exceções apr\n",
            "------------------------------------\n",
            "Executando nó de triagem...\n",
            "Decidindo após a triagem...\n",
            "Executando nó de auto_resolver...\n",
            "Decidindo após o auto_resolver...\n",
            "Rag com sucesso, finalizando o fluxo.\n",
            "PERGUNTA: É possível reembolsar certificações do Google Cloud?\n",
            "DECISÃO: AUTO_RESOLVER | URGÊNCIA: BAIXA | AÇÃO FINAL: AUTO_RESOLVER\n",
            "RESPOSTA: Sim, certificações são reembolsáveis, desde que haja aprovação prévia do gestor e orçamento do time.\n",
            "CITAÇÕES:\n",
            " - Documento: Política de Reembolsos (Viagens e Despesas).pdf, Página: 1\n",
            "   Trecho: Política de Reembolsos (Viagens e Despesas) 1.​ Reembolso: requer nota fiscal e deve ser submetido em até 10 dias corrid\n",
            " - Documento: Política de Uso de E-mail e Segurança da Informação.pdf, Página: 1\n",
            "   Trecho: 5.​ Solicitações de liberação de anexos ou domínios devem ser abertas por chamado, com justificativa do gestor.\n",
            "------------------------------------\n",
            "Executando nó de triagem...\n",
            "Decidindo após a triagem...\n",
            "Executando nó de pedir_info...\n",
            "PERGUNTA: Posso obter o Google Gemini de graça?\n",
            "DECISÃO: PEDIR_INFO | URGÊNCIA: BAIXA | AÇÃO FINAL: PEDIR_INFO\n",
            "RESPOSTA: Para avançar, preciso que detalhe: relevancia_para_politicas_internas\n",
            "------------------------------------\n",
            "Executando nó de triagem...\n",
            "Decidindo após a triagem...\n",
            "Executando nó de pedir_info...\n",
            "PERGUNTA: Qual é a palavra-chave da aula de hoje?\n",
            "DECISÃO: PEDIR_INFO | URGÊNCIA: BAIXA | AÇÃO FINAL: PEDIR_INFO\n",
            "RESPOSTA: Para avançar, preciso que detalhe: Tema e contexto específico\n",
            "------------------------------------\n",
            "Executando nó de triagem...\n",
            "Decidindo após a triagem...\n",
            "Executando nó de pedir_info...\n",
            "PERGUNTA: Quantas capivaras tem no Rio Pinheiros?\n",
            "DECISÃO: PEDIR_INFO | URGÊNCIA: BAIXA | AÇÃO FINAL: PEDIR_INFO\n",
            "RESPOSTA: Para avançar, preciso que detalhe: contexto_politica_interna\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código itera sobre uma lista de mensagens de teste e utiliza o grafo do LangGraph que você construiu para obter a resposta do agente para cada pergunta. Em seguida, ele imprime a pergunta original, a decisão da triagem, a ação final tomada pelo agente e a resposta gerada, incluindo citações se houver contexto relevante encontrado.\n",
        "\n",
        "Aqui está a análise:\n",
        "\n",
        "for msg_test in testes:: Inicia um loop for que irá percorrer cada item na lista chamada testes (definida na célula anterior). Em cada iteração do loop, o item atual da lista (uma string contendo uma pergunta de teste) será atribuído à variável msg_test.\n",
        "resposta_final = grafo.invoke({\"pergunta\": msg_test}): Esta é a linha que executa o seu agente LangGraph.\n",
        "grafo.invoke(...): Chama o método invoke no objeto grafo compilado. Este método inicia a execução do fluxo do LangGraph.\n",
        "{\"pergunta\": msg_test}: Passa um dicionário como entrada inicial para o grafo. A chave \"pergunta\" é usada para inicializar o estado do agente com a pergunta de teste atual (msg_test). O grafo começará a execução a partir do nó START, que está conectado ao nó triagem, e o state inicial conterá a pergunta.\n",
        "O resultado da execução completa do grafo é um dicionário (do tipo AgentState) que contém o estado final do agente após passar por todos os nós relevantes. Este dicionário final é armazenado na variável resposta_final.\n",
        "triag = resposta_final.get(\"triagem\", {}): Acessa o resultado da triagem do estado final. Ele usa .get(\"triagem\", {}) para obter o dicionário associado à chave \"triagem\". Se a chave não existir (o que não deve acontecer neste grafo, mas é uma prática segura), ele retorna um dicionário vazio {} como padrão.\n",
        "print(f\"PERGUNTA: {msg_test}\"): Imprime a pergunta de teste atual.\n",
        "print(f\"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}\"): Imprime informações resumidas sobre o resultado do fluxo:\n",
        "triag.get('decisao'): A decisão da triagem (AUTO_RESOLVER, PEDIR_INFO, ABRIR_CHAMADO).\n",
        "triag.get('urgencia'): O nível de urgência determinado pela triagem.\n",
        "resposta_final.get('acao_final'): A ação final tomada pelo agente (AUTO_RESOLVER, PEDIR_INFO, ABRIR_CHAMADO), conforme determinado pelos nós condicionais.\n",
        "print(f\"RESPOSTA: {resposta_final.get('resposta')}\"): Imprime a resposta gerada pelo agente, que pode ser a resposta do RAG, a mensagem solicitando mais informações ou a mensagem de abertura de chamado.\n",
        "if resposta_final.get(\"citacoes\"):: Verifica se a chave \"citacoes\" existe no estado final e se a lista de citações não está vazia. Isso indica que o nó auto_resolver encontrou contexto relevante e gerou citações.\n",
        "print(\"CITAÇÕES:\"): Se houver citações, imprime o cabeçalho \"CITAÇÕES:\".\n",
        "for citacao in resposta_final.get(\"citacoes\"):: Inicia um loop aninhado para iterar sobre cada dicionário de citação na lista.\n",
        "print(f\" - Documento: {citacao['documento']}, Página: {citacao['pagina']}\"): Imprime o nome do documento e o número da página da citação.\n",
        "print(f\" Trecho: {citacao['trecho']}\"): Imprime o trecho específico do documento que foi citado.\n",
        "print(\"------------------------------------\"): Imprime uma linha separadora após a saída de cada pergunta de teste para melhorar a legibilidade.\n",
        "Em resumo, este código executa o seu agente LangGraph para cada pergunta na lista testes e exibe o resultado completo do fluxo para cada pergunta, incluindo a triagem inicial, a ação final e a resposta gerada (com citações, se aplicável).\n",
        "\n",
        "Parece que você já executou este código com sucesso e viu as respostas do agente para as perguntas de teste!"
      ],
      "metadata": {
        "id": "qN5qQhuDgvja"
      }
    }
  ]
}